{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import sys\n",
    "import time\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import twitterquery as tq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Size Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "directory = '/mnt/66e695cd-1a0c-4e3b-9a50-55e01b788529/Training_data/Staging_final/'\n",
    "raw_data = tq.data.get_raw_data(spark, directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tweets: 135,910,871\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Tweets: {:,}\".format(raw_data.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topics = tq.utils.get_topics()\n",
    "topics_human_readable = tq.utils.get_readable_topic()\n",
    "topical_counts = pd.DataFrame(columns=['Topic', 'Positives', 'Total']).astype({'Positives': int, 'Total': int})\n",
    "for topic in topics:\n",
    "    labled_data = tq.data.load_labeled_data(spark, raw_data, topic)\n",
    "    topical_counts = topical_counts.append({'Topic': topics_human_readable[topic] if topic in topics_human_readable else topic,\n",
    "                                          'Positives': tq.data.get_num_of_positive_labels(labled_data),\n",
    "                                          'Total': labled_data.count()}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Positives</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Natural Disaster</td>\n",
       "      <td>89440</td>\n",
       "      <td>13877076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Topic  Positives     Total\n",
       "0  Natural Disaster      89440  13877076"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topical_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling Hashtags\n",
    "\n",
    "Some examples of the hashtags used for labeling each tweet as topical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topics = tq.settings.TOPICS\n",
    "topic_dict = tq.hashtag_dict.topic_dict\n",
    "hashtags_pd = pd.DataFrame()\n",
    "for topic in topics:\n",
    "    hashtags = random.sample(topic_dict[topic], 10)\n",
    "    hashtags = [h for h in hashtags]\n",
    "    hashtags_row = pd.DataFrame(hashtags)\n",
    "    hashtags_pd = pd.concat([hashtags_pd, hashtags_row], axis=1)\n",
    "    \n",
    "hashtags_pd.columns = [topics_human_readable[t] if t in topics_human_readable else t for t in topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Natural Disaster</th>\n",
       "      <th>Social Issues</th>\n",
       "      <th>Space</th>\n",
       "      <th>Soccer</th>\n",
       "      <th>Human Disasters</th>\n",
       "      <th>Tennis</th>\n",
       "      <th>Health</th>\n",
       "      <th>LGBT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tsunami</td>\n",
       "      <td>aurarosser</td>\n",
       "      <td>lunar</td>\n",
       "      <td>usavsger</td>\n",
       "      <td>israel</td>\n",
       "      <td>wimbledonfinals</td>\n",
       "      <td>eplague</td>\n",
       "      <td>gaymoment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chileearthquake</td>\n",
       "      <td>ericgarner</td>\n",
       "      <td>planetsunburn</td>\n",
       "      <td>sportsbetting</td>\n",
       "      <td>igad</td>\n",
       "      <td>wimbledon2013</td>\n",
       "      <td>theplague</td>\n",
       "      <td>sacksheila</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hurricanekatrina</td>\n",
       "      <td>debts</td>\n",
       "      <td>apollo11</td>\n",
       "      <td>worldcup2014brazil</td>\n",
       "      <td>jamesfoley</td>\n",
       "      <td>atpworldtourfinals</td>\n",
       "      <td>uniteblue</td>\n",
       "      <td>loveislove</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sandy</td>\n",
       "      <td>moa</td>\n",
       "      <td>cometlanding</td>\n",
       "      <td>dortmund</td>\n",
       "      <td>freesyria</td>\n",
       "      <td>teamnovak</td>\n",
       "      <td>factsnotfear</td>\n",
       "      <td>p2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hurricanekid</td>\n",
       "      <td>thisstopstoday</td>\n",
       "      <td>planets</td>\n",
       "      <td>ballislife</td>\n",
       "      <td>malala</td>\n",
       "      <td>rafanadaltour</td>\n",
       "      <td>nursesfightebola</td>\n",
       "      <td>equalityformen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>flood2013</td>\n",
       "      <td>mikebrown</td>\n",
       "      <td>antares</td>\n",
       "      <td>socce</td>\n",
       "      <td>antiwar</td>\n",
       "      <td>tenniscourt</td>\n",
       "      <td>humanitarianheroes</td>\n",
       "      <td>homos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tsunami2004</td>\n",
       "      <td>freetheweed</td>\n",
       "      <td>iris</td>\n",
       "      <td>mu</td>\n",
       "      <td>234whitegirls</td>\n",
       "      <td>cincytennis</td>\n",
       "      <td>depressionawareness</td>\n",
       "      <td>ccot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tsunami4nayapakistan</td>\n",
       "      <td>policelivesmatter</td>\n",
       "      <td>meteorjs</td>\n",
       "      <td>nufc</td>\n",
       "      <td>stayoutofsyria</td>\n",
       "      <td>teamdjokovic</td>\n",
       "      <td>publichealth</td>\n",
       "      <td>equalityforall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>laquake</td>\n",
       "      <td>antoniomartin</td>\n",
       "      <td>science</td>\n",
       "      <td>soccergrlprobs</td>\n",
       "      <td>famine</td>\n",
       "      <td>frenchopen2013</td>\n",
       "      <td>stopebola</td>\n",
       "      <td>2a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>earthquake</td>\n",
       "      <td>nonewnjgunlaws</td>\n",
       "      <td>meteors</td>\n",
       "      <td>league1</td>\n",
       "      <td>notinmyname</td>\n",
       "      <td>frenchopentennis</td>\n",
       "      <td>virus</td>\n",
       "      <td>acceptancematters</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Natural Disaster      Social Issues          Space              Soccer  \\\n",
       "0               tsunami         aurarosser          lunar            usavsger   \n",
       "1       chileearthquake         ericgarner  planetsunburn       sportsbetting   \n",
       "2      hurricanekatrina              debts       apollo11  worldcup2014brazil   \n",
       "3                 sandy                moa   cometlanding            dortmund   \n",
       "4          hurricanekid     thisstopstoday        planets          ballislife   \n",
       "5             flood2013          mikebrown        antares               socce   \n",
       "6           tsunami2004        freetheweed           iris                  mu   \n",
       "7  tsunami4nayapakistan  policelivesmatter       meteorjs                nufc   \n",
       "8               laquake      antoniomartin        science      soccergrlprobs   \n",
       "9            earthquake     nonewnjgunlaws        meteors             league1   \n",
       "\n",
       "  Human Disasters              Tennis               Health               LGBT  \n",
       "0          israel     wimbledonfinals              eplague          gaymoment  \n",
       "1            igad       wimbledon2013            theplague         sacksheila  \n",
       "2      jamesfoley  atpworldtourfinals            uniteblue         loveislove  \n",
       "3       freesyria           teamnovak         factsnotfear                 p2  \n",
       "4          malala       rafanadaltour     nursesfightebola     equalityformen  \n",
       "5         antiwar         tenniscourt   humanitarianheroes              homos  \n",
       "6   234whitegirls         cincytennis  depressionawareness               ccot  \n",
       "7  stayoutofsyria        teamdjokovic         publichealth     equalityforall  \n",
       "8          famine      frenchopen2013            stopebola                 2a  \n",
       "9     notinmyname    frenchopentennis                virus  acceptancematters  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtags_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split and Save the Labeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kfold_split_directory = '/mnt/1e69d2b1-91a9-473c-a164-db90daf43a3d/splitted_data/kfold/'\n",
    "topics = tq.utils.get_topics()[4:]\n",
    "for topic in topics:\n",
    "    labeled_data = tq.data.get_labeled_data(raw_data, topic, frac=0.1)\n",
    "    splitted_data = tq.data.split_kfold(labeled_data, k=5)\n",
    "    for i, split in enumerate(splitted_data):\n",
    "        split_pd = split.toPandas().dropna()\n",
    "        output_path = kfold_split_directory + topic + \"/\" + str(i+1) + \".csv\"\n",
    "        split_pd.to_csv(output_path, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Topic: Social Issues\n"
     ]
    }
   ],
   "source": [
    "num_of_splits = 5\n",
    "num_of_features = 20\n",
    "\n",
    "for topic in tq.utils.get_topics():\n",
    "    print(\"Processing Topic: {}\".format(utils.get_readable_topic(topic)))\n",
    "    preprocess_start_time = time.time()    \n",
    "    data_features, data_labels, data_transformer = data.get_transformed_data(topic, \n",
    "                                                                             shuffle=True)\n",
    "    print(\"Data Preprocess in {:.2f} Seconds\".format(time.time() - preprocess_start_time))\n",
    "\n",
    "    # perform k fold evaluation\n",
    "    training_start_time = time.time()\n",
    "    firehose_phase1_stats = []\n",
    "    firehose_train_aveps = []\n",
    "    firehose_test_aveps = []\n",
    "    firehose_patks = []\n",
    "    \n",
    "    topk_feats = []\n",
    "    topk_phase1_stats = []\n",
    "    topk_train_aveps = []\n",
    "    topk_test_aveps = []\n",
    "    topk_patks = []\n",
    "    \n",
    "    gur_cilp_feats = []\n",
    "    gur_cilp_phase1_stats = []\n",
    "    gur_cilp_train_aveps = []\n",
    "    gur_cilp_test_aveps = []\n",
    "    gur_cilp_patks = []\n",
    "\n",
    "    greedy_cilp_feats = []\n",
    "    greedy_cilp_phase1_stats = []\n",
    "    greedy_cilp_train_aveps = []\n",
    "    greedy_cilp_test_aveps = []\n",
    "    greedy_cilp_patks = []\n",
    "    \n",
    "    gur_wilp_feats = []\n",
    "    gur_wilp_phase1_stats = []\n",
    "    gur_wilp_train_aveps = []\n",
    "    gur_wilp_test_aveps = []\n",
    "    gur_wilp_patks = []\n",
    "\n",
    "    greedy_wilp_feats = []\n",
    "    greedy_wilp_phase1_stats = []\n",
    "    greedy_wilp_train_aveps = []\n",
    "    greedy_wilp_test_aveps = []\n",
    "    greedy_wilp_patks = []\n",
    "    \n",
    "    gur_cailp_feats = []\n",
    "    gur_cailp_phase1_stats = []\n",
    "    gur_cailp_train_aveps = []\n",
    "    gur_cailp_test_aveps = []\n",
    "    gur_cailp_patks = []\n",
    "    \n",
    "    greedy_cailp_feats = []\n",
    "    greedy_cailp_phase1_stats = []\n",
    "    greedy_cailp_train_aveps = []\n",
    "    greedy_cailp_test_aveps = []\n",
    "    greedy_cailp_patks = []\n",
    "\n",
    "    for i in range(num_of_splits):    \n",
    "        print(\"===split {}/{}===\".format(i+1, num_of_splits))\n",
    "        print(\"getting train/test data\")\n",
    "        start_time = time.time()\n",
    "        (train_data, train_target\n",
    "        , test_data, test_target) = transformers.train_test_split(data_features,\n",
    "                                                                 data_labels,\n",
    "                                                                 test_split_index=i,\n",
    "                                                                 num_of_splits=num_of_splits)\n",
    "        firehose_phase1_stats.append(utils.get_labeled_data_statistics(train_target))\n",
    "        print(\"split done in {:.2f}s\".format(time.time() - start_time))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        print(\"training firehose classifier\")\n",
    "        start_time = time.time()\n",
    "        firehose_classifier = SGDClassifier(loss='log', class_weight='balanced', penalty='elasticnet')\n",
    "        firehose_classifier.fit(train_data, train_target)          \n",
    "        preds_proba = firehose_classifier.predict_proba(train_data)[:, 1]\n",
    "        firehose_train_aveps.append(average_precision_score(train_target, preds_proba))\n",
    "        preds_proba = firehose_classifier.predict_proba(test_data)[:, 1]\n",
    "        firehose_test_aveps.append(average_precision_score(test_target, preds_proba))\n",
    "        firehose_patks.append(classification.p_at_k_score(test_target, preds_proba, k=100))\n",
    "\n",
    "        print(\"firehose classifier trained in {:.2f}s\".format(time.time() - start_time))\n",
    "\n",
    "        # topk \n",
    "        print(\"**TopK\")\n",
    "        topk_feat_index = classification.get_classifier_top_weighted_features(firehose_classifier, k=num_of_features)\n",
    "        topk_feats.append(data_transformer.get_features_by_index(topk_feat_index))\n",
    "        ph1, train_avep, test_avep, patk = utils.run_experiment(train_data, train_target, \n",
    "                                                                test_data, test_target,\n",
    "                                                                topk_feat_index)\n",
    "        topk_phase1_stats.append(ph1)\n",
    "        topk_train_aveps.append(train_avep)\n",
    "        topk_test_aveps.append(test_avep)\n",
    "        topk_patks.append(patk)\n",
    "        \n",
    "        # MILP formulations\n",
    "        positive_set, negative_set = transformers.get_positive_negative_set(test_data, test_target)\n",
    "        \n",
    "        # gurobi_cilp \n",
    "        print(\"**Gurobi CILP\")\n",
    "        gur_cilp_feat_index = milp.gurobi_cilp(positive_set, k=num_of_features)\n",
    "        gur_cilp_feats.append(data_transformer.get_features_by_index(gur_cilp_feat_index))\n",
    "        ph1, train_avep, test_avep, patk = utils.run_experiment(train_data, train_target, \n",
    "                                                                test_data, test_target,\n",
    "                                                                gur_cilp_feat_index)\n",
    "        gur_cilp_phase1_stats.append(ph1)\n",
    "        gur_cilp_train_aveps.append(train_avep)\n",
    "        gur_cilp_test_aveps.append(test_avep)\n",
    "        gur_cilp_patks.append(patk)\n",
    "        \n",
    "        # greedy_cilp\n",
    "        print(\"**Greedy CILP\")\n",
    "        greedy_cilp_feat_index = milp.greedy_cilp(positive_set, k=num_of_features)\n",
    "        greedy_cilp_feats.append(data_transformer.get_features_by_index(greedy_cilp_feat_index))\n",
    "        ph1, train_avep, test_avep, patk = utils.run_experiment(train_data, train_target, \n",
    "                                                                test_data, test_target,\n",
    "                                                                greedy_cilp_feat_index)\n",
    "        greedy_cilp_phase1_stats.append(ph1)\n",
    "        greedy_cilp_train_aveps.append(train_avep)\n",
    "        greedy_cilp_test_aveps.append(test_avep)\n",
    "        greedy_cilp_patks.append(patk)\n",
    "        \n",
    "        # get mi scores\n",
    "        print(\"**Getting MI Scores\")\n",
    "        start_time = time.time()\n",
    "        mi_scores = data.get_mi_scores(test_data, test_target)\n",
    "        print(\"MI Scores done in {:.2f} seconds\".format(time.time() - start_time))\n",
    "        \n",
    "        # gurobi_wilp\n",
    "        print(\"**Gurobi WILP\")\n",
    "        gur_wilp_feat_index = milp.gurobi_wilp(positive_set, mi_scores, k=num_of_features)\n",
    "        gur_wilp_feats.append(data_transformer.get_features_by_index(gur_wilp_feat_index))\n",
    "        ph1, train_avep, test_avep, patk = utils.run_experiment(train_data, train_target, \n",
    "                                                                test_data, test_target,\n",
    "                                                                gur_wilp_feat_index)\n",
    "        gur_wilp_phase1_stats.append(ph1)\n",
    "        gur_wilp_train_aveps.append(train_avep)\n",
    "        gur_wilp_test_aveps.append(test_avep)\n",
    "        gur_wilp_patks.append(patk)\n",
    "        \n",
    "        # greedy_wilp\n",
    "        print(\"**Greedy WILP\")\n",
    "        greedy_wilp_feat_index = milp.greedy_wilp(positive_set, mi_scores, k=num_of_features)\n",
    "        greedy_wilp_feats.append(data_transformer.get_features_by_index(greedy_wilp_feat_index))\n",
    "        ph1, train_avep, test_avep, patk = utils.run_experiment(train_data, train_target, \n",
    "                                                                test_data, test_target,\n",
    "                                                                greedy_wilp_feat_index)\n",
    "        greedy_wilp_phase1_stats.append(ph1)\n",
    "        greedy_wilp_train_aveps.append(train_avep)\n",
    "        greedy_wilp_test_aveps.append(test_avep)\n",
    "        greedy_wilp_patks.append(patk)\n",
    "        \n",
    "        # gurobi cailp\n",
    "        print(\"**Gurobi CAILP\")\n",
    "        start_time = time.time()\n",
    "        gur_cailp_feat_index = milp.gurobi_cailp(positive_set, negative_set, k=num_of_features)\n",
    "        print(\"Gur CAILP in {:.2f} seconds\".format(time.time() - start_time))\n",
    "        gur_cailp_feats.append(data_transformer.get_features_by_index(gur_cailp_feat_index))\n",
    "        ph1, train_avep, test_avep, patk = utils.run_experiment(train_data, train_target, \n",
    "                                                                test_data, test_target,\n",
    "                                                                gur_cailp_feat_index)\n",
    "        gur_cailp_phase1_stats.append(ph1)\n",
    "        gur_cailp_train_aveps.append(train_avep)\n",
    "        gur_cailp_test_aveps.append(test_avep)\n",
    "        gur_cailp_patks.append(patk)\n",
    "        \n",
    "        # greedy cailp\n",
    "        print(\"**Greedy CAILP\")\n",
    "        start_time = time.time()\n",
    "        greedy_cailp_feat_index = milp.greedy_cailp(positive_set, negative_set, k=num_of_features)\n",
    "        print(\"Greedy CAILP in {:.2f} seconds\".format(time.time() - start_time))\n",
    "        greedy_cailp_feats.append(data_transformer.get_features_by_index(greedy_cailp_feat_index))\n",
    "        ph1, train_avep, test_avep, patk = utils.run_experiment(train_data, train_target, \n",
    "                                                                test_data, test_target,\n",
    "                                                                greedy_cailp_feat_index)\n",
    "        greedy_cailp_phase1_stats.append(ph1)\n",
    "        greedy_cailp_train_aveps.append(train_avep)\n",
    "        greedy_cailp_test_aveps.append(test_avep)\n",
    "        greedy_cailp_patks.append(patk)\n",
    "        \n",
    "    print(\"Training Done in {:.2f} Seconds\".format(time.time() - training_start_time))\n",
    "    print(\"******\")\n",
    "    \n",
    "    utils.save_results_to_csv(topic, \n",
    "                          'firehose', \n",
    "                          firehose_phase1_stats, \n",
    "                          firehose_train_aveps,\n",
    "                          firehose_test_aveps,\n",
    "                          firehose_patks)\n",
    "    \n",
    "    utils.save_results_to_csv(topic, \n",
    "                          'topk', \n",
    "                          topk_phase1_stats, \n",
    "                          topk_train_aveps,\n",
    "                          topk_test_aveps,\n",
    "                          topk_patks,\n",
    "                          topk_feats)\n",
    "    \n",
    "    utils.save_results_to_csv(topic, \n",
    "                          'gurobi_cilp', \n",
    "                          gur_cilp_phase1_stats, \n",
    "                          gur_cilp_train_aveps,\n",
    "                          gur_cilp_test_aveps,\n",
    "                          gur_cilp_patks,\n",
    "                          gur_cilp_feats)\n",
    "    \n",
    "    utils.save_results_to_csv(topic, \n",
    "                      'greedy_cilp', \n",
    "                      greedy_cilp_phase1_stats, \n",
    "                      greedy_cilp_train_aveps,\n",
    "                      greedy_cilp_test_aveps,\n",
    "                      greedy_cilp_patks,\n",
    "                      greedy_cilp_feats)\n",
    "    \n",
    "    utils.save_results_to_csv(topic, \n",
    "                      'gurobi_wilp', \n",
    "                      gur_wilp_phase1_stats, \n",
    "                      gur_wilp_train_aveps,\n",
    "                      gur_wilp_test_aveps,\n",
    "                      gur_wilp_patks,\n",
    "                      gur_wilp_feats)\n",
    "    \n",
    "    utils.save_results_to_csv(topic, \n",
    "                  'greedy_wilp', \n",
    "                  greedy_wilp_phase1_stats, \n",
    "                  greedy_wilp_train_aveps,\n",
    "                  greedy_wilp_test_aveps,\n",
    "                  greedy_wilp_patks,\n",
    "                  greedy_wilp_feats)\n",
    "    \n",
    "    utils.save_results_to_csv(topic, \n",
    "                      'gurobi_cailp', \n",
    "                      gur_cailp_phase1_stats, \n",
    "                      gur_cailp_train_aveps,\n",
    "                      gur_cailp_test_aveps,\n",
    "                      gur_cailp_patks,\n",
    "                      gur_cailp_feats)\n",
    "    \n",
    "    utils.save_results_to_csv(topic, \n",
    "                      'greedy_cailp', \n",
    "                      greedy_cailp_phase1_stats, \n",
    "                      greedy_cailp_train_aveps,\n",
    "                      greedy_cailp_test_aveps,\n",
    "                      greedy_cailp_patks,\n",
    "                      greedy_cailp_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Varying K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Topic: Space\n",
      "filtered in 1354.32101202\n",
      "===split 2/5===\n",
      "getting train/test data\n",
      "split done in 3.40s\n",
      "K = 10\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "topics = [#'Social_issue',\n",
    "         'Space',\n",
    "         'Soccer',\n",
    "         'Human_Disaster',\n",
    "         'Tennis',\n",
    "         'Health',\n",
    "         'LGBT']\n",
    "ks = [10, 20, 50, 100, 200]\n",
    "num_of_splits = 5\n",
    "\n",
    "for topic in topics:\n",
    "    print(\"Processing Topic: {}\".format(tq.utils.get_readable_topic(topic)))\n",
    "    preprocess_start_time = time.time()    \n",
    "    data_features, data_labels, data_transformer = tq.data.get_transformed_data(topic, \n",
    "                                                                             shuffle=True)\n",
    "    print(\"Data Preprocess in {:.2f} Seconds\".format(time.time() - preprocess_start_time))\n",
    "    \n",
    "    tots = []\n",
    "    gurobi_stats = []\n",
    "    greedy_stats = []\n",
    "    for k in ks:\n",
    "        gurobi_stats.append([])\n",
    "        greedy_stats.append([])        \n",
    "    for i in range(num_of_splits):    \n",
    "        print(\"===split {}/{}===\".format(i+1, num_of_splits))\n",
    "        print(\"getting train/test data\")\n",
    "        start_time = time.time()\n",
    "        (train_data, train_target\n",
    "        , test_data, test_target) = transformers.train_test_split(data_features,\n",
    "                                                                 data_labels,\n",
    "                                                                 test_split_index=i,\n",
    "                                                                 num_of_splits=num_of_splits)\n",
    "        tots.append(utils.get_labeled_data_statistics(train_target))\n",
    "        print(\"split done in {:.2f}s\".format(time.time() - start_time))\n",
    "        positive_set, negative_set = transformers.get_positive_negative_set(test_data, test_target, pos_count=3000) \n",
    "        for i, k in enumerate(ks):\n",
    "            print(\"K = {}\".format(k))\n",
    "            start_time = time.time()\n",
    "            gurobi_cailp_feat_index = milp.gurobi_cailp(positive_set, negative_set, k=k)\n",
    "            _, filtered_train_labels = data.filter_matrix_by_index(train_data,\n",
    "                                                                   train_target,\n",
    "                                                                   gurobi_cailp_feat_index) \n",
    "            gurobi_stats[i].append(utils.get_labeled_data_statistics(filtered_train_labels))\n",
    "            greedy_cailp_feat_index = milp.greedy_cailp(positive_set, negative_set, k=k)\n",
    "            _, filtered_train_labels = data.filter_matrix_by_index(train_data,\n",
    "                                                                   train_target,\n",
    "                                                                   greedy_cailp_feat_index) \n",
    "\n",
    "            greedy_stats[i].append(utils.get_labeled_data_statistics(filtered_train_labels))\n",
    "            print(\"filtered in {}\".format(time.time() - start_time))\n",
    "    with open('../results/2019/' + topic + '/gurobi_vark.pickle', 'wb') as f:\n",
    "        pickle.dump(gurobi_stats, f)\n",
    "    with open('../results/2019/' + topic + '/greedy_vark.pickle', 'wb') as f:\n",
    "        pickle.dump(greedy_stats, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall@n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================\n",
      "Processing Topic: Soccer\n",
      "Data Preprocess in 732.70 Seconds\n",
      "getting train/test data\n",
      "training firehose classifier\n",
      "firehose classifier trained in 26.35s\n",
      "**TopK\n",
      "** CILP\n",
      "model built\n",
      "**Getting MI Scores\n",
      "number of features = 75537\n",
      "MI Scores done in 78.41 seconds\n",
      "** WILP\n",
      "**CAILP\n",
      "getting train/test data\n",
      "training firehose classifier\n",
      "firehose classifier trained in 25.76s\n",
      "**TopK\n",
      "** CILP\n",
      "model built\n",
      "**Getting MI Scores\n",
      "number of features = 75537\n",
      "MI Scores done in 76.22 seconds\n",
      "** WILP\n",
      "**CAILP\n",
      "getting train/test data\n",
      "training firehose classifier\n",
      "firehose classifier trained in 25.68s\n",
      "**TopK\n",
      "** CILP\n",
      "model built\n",
      "**Getting MI Scores\n",
      "number of features = 75537\n",
      "MI Scores done in 63.18 seconds\n",
      "** WILP\n",
      "**CAILP\n",
      "getting train/test data\n",
      "training firehose classifier\n",
      "firehose classifier trained in 25.88s\n",
      "**TopK\n",
      "** CILP\n",
      "model built\n",
      "**Getting MI Scores\n",
      "number of features = 75537\n",
      "MI Scores done in 63.27 seconds\n",
      "** WILP\n",
      "**CAILP\n",
      "getting train/test data\n",
      "training firehose classifier\n",
      "firehose classifier trained in 25.53s\n",
      "**TopK\n",
      "** CILP\n",
      "model built\n",
      "**Getting MI Scores\n",
      "number of features = 75537\n",
      "MI Scores done in 63.01 seconds\n",
      "** WILP\n",
      "**CAILP\n",
      "===========================\n",
      "Processing Topic: Human Disasters\n",
      "Data Preprocess in 708.59 Seconds\n",
      "getting train/test data\n",
      "training firehose classifier\n",
      "firehose classifier trained in 24.35s\n",
      "**TopK\n",
      "** CILP\n",
      "model built\n",
      "**Getting MI Scores\n",
      "number of features = 74207\n",
      "MI Scores done in 62.19 seconds\n",
      "** WILP\n",
      "**CAILP\n",
      "getting train/test data\n",
      "training firehose classifier\n",
      "firehose classifier trained in 24.48s\n",
      "**TopK\n",
      "** CILP\n",
      "model built\n",
      "**Getting MI Scores\n",
      "number of features = 74207\n",
      "MI Scores done in 63.59 seconds\n",
      "** WILP\n",
      "**CAILP\n",
      "getting train/test data\n",
      "training firehose classifier\n",
      "firehose classifier trained in 24.53s\n",
      "**TopK\n",
      "** CILP\n",
      "model built\n",
      "**Getting MI Scores\n",
      "number of features = 74207\n",
      "MI Scores done in 62.99 seconds\n",
      "** WILP\n",
      "**CAILP\n",
      "getting train/test data\n",
      "training firehose classifier\n",
      "firehose classifier trained in 25.31s\n",
      "**TopK\n",
      "** CILP\n",
      "model built\n",
      "**Getting MI Scores\n",
      "number of features = 74207\n",
      "MI Scores done in 63.14 seconds\n",
      "** WILP\n",
      "**CAILP\n",
      "getting train/test data\n",
      "training firehose classifier\n",
      "firehose classifier trained in 25.13s\n",
      "**TopK\n",
      "** CILP\n",
      "model built\n",
      "**Getting MI Scores\n",
      "number of features = 74207\n",
      "MI Scores done in 62.85 seconds\n",
      "** WILP\n",
      "**CAILP\n",
      "===========================\n",
      "Processing Topic: Tennis\n",
      "Data Preprocess in 706.88 Seconds\n",
      "getting train/test data\n",
      "training firehose classifier\n",
      "firehose classifier trained in 22.84s\n",
      "**TopK\n",
      "** CILP\n",
      "model built\n",
      "**Getting MI Scores\n",
      "number of features = 71352\n",
      "MI Scores done in 60.31 seconds\n",
      "** WILP\n",
      "**CAILP\n",
      "getting train/test data\n",
      "training firehose classifier\n",
      "firehose classifier trained in 23.04s\n",
      "**TopK\n",
      "** CILP\n",
      "model built\n",
      "**Getting MI Scores\n",
      "number of features = 71352\n",
      "MI Scores done in 60.38 seconds\n",
      "** WILP\n",
      "**CAILP\n",
      "getting train/test data\n",
      "training firehose classifier\n",
      "firehose classifier trained in 23.08s\n",
      "**TopK\n",
      "** CILP\n",
      "model built\n",
      "**Getting MI Scores\n",
      "number of features = 71352\n",
      "MI Scores done in 63.32 seconds\n",
      "** WILP\n",
      "**CAILP\n",
      "getting train/test data\n",
      "training firehose classifier\n",
      "firehose classifier trained in 23.10s\n",
      "**TopK\n",
      "** CILP\n",
      "model built\n",
      "**Getting MI Scores\n",
      "number of features = 71352\n",
      "MI Scores done in 59.63 seconds\n",
      "** WILP\n",
      "**CAILP\n",
      "getting train/test data\n",
      "training firehose classifier\n",
      "firehose classifier trained in 23.64s\n",
      "**TopK\n",
      "** CILP\n",
      "model built\n",
      "**Getting MI Scores\n",
      "number of features = 71352\n",
      "MI Scores done in 60.54 seconds\n",
      "** WILP\n",
      "**CAILP\n",
      "===========================\n",
      "Processing Topic: Health\n",
      "Data Preprocess in 692.89 Seconds\n",
      "getting train/test data\n",
      "training firehose classifier\n",
      "firehose classifier trained in 23.97s\n",
      "**TopK\n",
      "** CILP\n",
      "model built\n",
      "**Getting MI Scores\n",
      "number of features = 72790\n",
      "MI Scores done in 60.80 seconds\n",
      "** WILP\n",
      "**CAILP\n",
      "getting train/test data\n",
      "training firehose classifier\n",
      "firehose classifier trained in 25.09s\n",
      "**TopK\n",
      "** CILP\n",
      "model built\n",
      "**Getting MI Scores\n",
      "number of features = 72790\n",
      "MI Scores done in 61.52 seconds\n",
      "** WILP\n",
      "**CAILP\n",
      "getting train/test data\n",
      "training firehose classifier\n",
      "firehose classifier trained in 25.05s\n",
      "**TopK\n",
      "** CILP\n",
      "model built\n",
      "**Getting MI Scores\n",
      "number of features = 72790\n",
      "MI Scores done in 60.75 seconds\n",
      "** WILP\n",
      "**CAILP\n",
      "getting train/test data\n",
      "training firehose classifier\n",
      "firehose classifier trained in 25.14s\n",
      "**TopK\n",
      "** CILP\n",
      "model built\n",
      "**Getting MI Scores\n",
      "number of features = 72790\n",
      "MI Scores done in 62.33 seconds\n",
      "** WILP\n",
      "**CAILP\n",
      "getting train/test data\n",
      "training firehose classifier\n",
      "firehose classifier trained in 25.03s\n",
      "**TopK\n",
      "** CILP\n",
      "model built\n",
      "**Getting MI Scores\n",
      "number of features = 72790\n",
      "MI Scores done in 61.56 seconds\n",
      "** WILP\n",
      "**CAILP\n",
      "===========================\n",
      "Processing Topic: LGBT\n",
      "Data Preprocess in 712.91 Seconds\n",
      "getting train/test data\n",
      "training firehose classifier\n",
      "firehose classifier trained in 24.05s\n",
      "**TopK\n",
      "** CILP\n",
      "model built\n",
      "**Getting MI Scores\n",
      "number of features = 74024\n",
      "MI Scores done in 62.58 seconds\n",
      "** WILP\n",
      "**CAILP\n",
      "getting train/test data\n",
      "training firehose classifier\n",
      "firehose classifier trained in 23.97s\n",
      "**TopK\n",
      "** CILP\n",
      "model built\n",
      "**Getting MI Scores\n",
      "number of features = 74024\n",
      "MI Scores done in 62.51 seconds\n",
      "** WILP\n",
      "**CAILP\n",
      "getting train/test data\n",
      "training firehose classifier\n",
      "firehose classifier trained in 24.44s\n",
      "**TopK\n",
      "** CILP\n",
      "model built\n",
      "**Getting MI Scores\n",
      "number of features = 74024\n",
      "MI Scores done in 61.59 seconds\n",
      "** WILP\n",
      "**CAILP\n",
      "getting train/test data\n",
      "training firehose classifier\n",
      "firehose classifier trained in 24.41s\n",
      "**TopK\n",
      "** CILP\n",
      "model built\n",
      "**Getting MI Scores\n",
      "number of features = 74024\n",
      "MI Scores done in 64.34 seconds\n",
      "** WILP\n",
      "**CAILP\n",
      "getting train/test data\n",
      "training firehose classifier\n",
      "firehose classifier trained in 24.49s\n",
      "**TopK\n",
      "** CILP\n",
      "model built\n",
      "**Getting MI Scores\n",
      "number of features = 74024\n",
      "MI Scores done in 61.79 seconds\n",
      "** WILP\n",
      "**CAILP\n"
     ]
    }
   ],
   "source": [
    "num_of_splits = 5\n",
    "num_of_features = 20\n",
    "n = 18000\n",
    "\n",
    "for topic in tq.utils.get_topics()[3:]:\n",
    "    print(\"===========================\")\n",
    "    print(\"Processing Topic: {}\".format(tq.utils.get_readable_topic(topic)))\n",
    "    preprocess_start_time = time.time()    \n",
    "    data_features, data_labels, data_transformer = tq.data.get_transformed_data(topic, \n",
    "                                                                             shuffle=True)\n",
    "    print(\"Data Preprocess in {:.2f} Seconds\".format(time.time() - preprocess_start_time))\n",
    "    \n",
    "    total_positives_in_split = []\n",
    "    firehose_positive_at_ns = []\n",
    "    topk_positive_at_ns = []\n",
    "    cilp_positive_at_ns = []\n",
    "    wilp_positive_at_ns = []\n",
    "    cailp_positive_at_ns = []\n",
    "\n",
    "    for i in range(num_of_splits):    \n",
    "        print(\"getting train/test data\")\n",
    "        start_time = time.time()\n",
    "        (train_data, train_target\n",
    "        , test_data, test_target) = tq.transformers.train_test_split(data_features,\n",
    "                                                                 data_labels,\n",
    "                                                                 test_split_index=i,\n",
    "                                                                 num_of_splits=num_of_splits,\n",
    "                                                                 include_timestamp=True)\n",
    "\n",
    "        total_positives_in_split.append(sum(test_target.label))\n",
    "        firehose_positive_at_ns.append(tq.data.count_most_recent_topcial_in_n(test_target, n=n))\n",
    "\n",
    "        print(\"training firehose classifier\")\n",
    "        start_time = time.time()\n",
    "        firehose_classifier = SGDClassifier(loss='log', class_weight='balanced', penalty='elasticnet')\n",
    "        firehose_classifier.fit(train_data, train_target.label)          \n",
    "\n",
    "        print(\"firehose classifier trained in {:.2f}s\".format(time.time() - start_time))\n",
    "\n",
    "        # topk \n",
    "        print(\"**TopK\")\n",
    "        topk_feat_index = tq.classification.get_classifier_top_weighted_features(firehose_classifier, k=num_of_features)\n",
    "        pos_at_n, total_count = tq.experiments.run_ratk_experiment(test_data, test_target, topk_feat_index, n=n)\n",
    "        topk_positive_at_ns.append(pos_at_n)\n",
    "\n",
    "        # MILP formulations\n",
    "        positive_set, negative_set = tq.transformers.get_positive_negative_set(train_data,\n",
    "                                                                               train_target.label,\n",
    "                                                                               pos_count=20000)\n",
    "\n",
    "        print(\"** CILP\")\n",
    "        cilp_feat_index = tq.milp.gurobi_cilp(positive_set, k=num_of_features)\n",
    "        pos_at_n, total_count = tq.experiments.run_ratk_experiment(test_data, test_target, cilp_feat_index, n=n)\n",
    "        cilp_positive_at_ns.append(pos_at_n)\n",
    "\n",
    "        print(\"**Getting MI Scores\")\n",
    "        start_time = time.time()\n",
    "        mi_scores = tq.data.get_mi_scores(train_data, train_target.label)\n",
    "        print(\"MI Scores done in {:.2f} seconds\".format(time.time() - start_time))\n",
    "\n",
    "        print(\"** WILP\")\n",
    "        wilp_feat_index = tq.milp.gurobi_wilp(positive_set, mi_scores, k=num_of_features)\n",
    "        pos_at_n, total_count = tq.experiments.run_ratk_experiment(test_data, test_target, wilp_feat_index, n=n)\n",
    "        wilp_positive_at_ns.append(pos_at_n)\n",
    "\n",
    "        print(\"**CAILP\")\n",
    "        cailp_feat_index = tq.milp.gurobi_cailp(positive_set, negative_set, k=num_of_features)\n",
    "        pos_at_n, total_count = tq.experiments.run_ratk_experiment(test_data, test_target, cailp_feat_index, n=n)\n",
    "        cailp_positive_at_ns.append(pos_at_n)\n",
    "\n",
    "    ratk_stats = {'firehose': firehose_positive_at_ns,\n",
    "         'topk': topk_positive_at_ns,\n",
    "         'cilp': cilp_positive_at_ns,\n",
    "         'wilp': wilp_positive_at_ns,\n",
    "         'cailp': cailp_positive_at_ns,\n",
    "         'total': total_positives_in_split}\n",
    "    \n",
    "    with open('../results/2019/' + topic + '/ratk_results.pickle', 'wb') as f:\n",
    "        pickle.dump(ratk_stats, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
