{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, mutual_info_score, normalized_mutual_info_score\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, IDF, StopWordsRemover, CountVectorizer, VectorAssembler\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import explode, lit, col, when, lower\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from hashtag_dict import topic_dict\n",
    "\n",
    "from Evaluation import *\n",
    "from FeatureExtraction import *\n",
    "from Filter import *\n",
    "from TestFramework import *\n",
    "from LPFormulations import *\n",
    "from CoverageCalculator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class API:\n",
    "    def __init__(self, directory, frac=1, seed=123):\n",
    "        if frac < 1:\n",
    "            self.data = spark.read.parquet(directory).sample(False, frac, seed)\n",
    "        else:\n",
    "            self.data = spark.read.parquet(directory)\n",
    "          \n",
    "    def get_raw_data(self):\n",
    "         return self.data\n",
    "\n",
    "    def get_filtered_data(self, terms=[], hashtags=None, users=None, locs=None, mentions=None):   \n",
    "        return filter_data(self.data, terms, hashtags, users, locs, mentions)\n",
    "    \n",
    "    \n",
    "class DataPreprocessor():\n",
    "    def __init__(self, data, topic, load_labled=False, filter_empty_hashtags=False):\n",
    "        if filter_empty_hashtags == True:\n",
    "            self.data = data.filter(col(\"hashtag\") != \"empty_hashtag\")\n",
    "        else:\n",
    "            self.data = data\n",
    "\n",
    "        if load_labled == True:\n",
    "            labled_dir = \"/mnt/1e69d2b1-91a9-473c-a164-db90daf43a3d/labeled_data/\" + topic\n",
    "            pos_dir = labled_dir + \"/pos\"\n",
    "            topical_tweet_ids = spark.read.parquet(pos_dir)\n",
    "            topical_tweets = topical_tweet_ids.join(self.data, \"tweet_id\").withColumn(\"label\", lit(1))\n",
    "            \n",
    "            neg_dir = labled_dir + \"/neg\"\n",
    "            non_topical_tweet_ids = spark.read.parquet(neg_dir).sample(False, 0.1, 123)\n",
    "            non_topical_tweets = non_topical_tweet_ids.join(self.data, \"tweet_id\").withColumn(\"label\", lit(0))\n",
    "\n",
    "            self.labled_data = topical_tweets.union(non_topical_tweets)\n",
    "        else:\n",
    "            self.labled_data = None\n",
    "\n",
    "        tokenizer = Tokenizer(inputCol=\"hashtag\", outputCol=\"each_hashtag\")\n",
    "        hashtags_df = tokenizer.transform(self.data)\n",
    "\n",
    "        hashtag = hashtags_df.select(\"tweet_id\",\"create_time\",\"each_hashtag\")\n",
    "        self.hash_exploded = hashtag.withColumn('each_hashtag', explode('each_hashtag'))\n",
    "    \n",
    "    def get_labled_data(self, topic):\n",
    "        if self.labled_data == None:\n",
    "            topic_lables = topic_dict[topic]\n",
    "\n",
    "            topical_ids = self.hash_exploded.select(self.hash_exploded.tweet_id)\\\n",
    "                                                    .where(self.hash_exploded.each_hashtag\\\n",
    "                                                            .isin(topic_lables))\\\n",
    "                                                    .distinct().cache()\n",
    "\n",
    "            labled_topical = topical_ids.withColumn(\"topical\", lit(1))\n",
    "            self.labled_data = self.data.join(labled_topical, self.data.tweet_id == labled_topical.tweet_id, \"left\")\\\n",
    "                                                      .select(self.data.create_time,\\\n",
    "                                                             self.data.from_id,\\\n",
    "                                                             self.data.from_user,\\\n",
    "                                                             self.data.hashtag,\\\n",
    "                                                             self.data.location,\\\n",
    "                                                             self.data.mention,\\\n",
    "                                                             self.data.tweet_id,\\\n",
    "                                                             self.data.term,\\\n",
    "                                                              F.when(labled_topical.topical == 1, 1.0)\\\n",
    "                                                                .otherwise(0.0).alias(\"label\"))\n",
    "        return self.labled_data\n",
    "    \n",
    "    def get_num_of_positvies(self):\n",
    "        if self.labled_data == None:\n",
    "            return 0\n",
    "        \n",
    "        return self.labled_data.where(col(\"label\") == 1.0).count()\n",
    "    \n",
    "    def get_data_size(self):\n",
    "        if self.labled_data == None:\n",
    "            return 0\n",
    "        \n",
    "        return self.labled_data.count()\n",
    "        \n",
    "    def temporal_split(self, topic, balance_data=True, seed=0, remove_topic_hashtags=False, ch=False):\n",
    "        topic_lables = topic_dict[topic]\n",
    "        labled_data = self.get_labled_data(topic)\n",
    "        tweet_labels = labled_data.select(\"tweet_id\", \"label\")\n",
    "        \n",
    "        df_birthday = self.hash_exploded.join(tweet_labels,\\\n",
    "                                 self.hash_exploded.tweet_id == tweet_labels.tweet_id,\\\n",
    "                                 \"inner\").select(self.hash_exploded.create_time,\\\n",
    "                                                 self.hash_exploded.each_hashtag,\\\n",
    "                                                 self.hash_exploded.tweet_id)\n",
    "        \n",
    "        ## Find out the \"birthday\", or the earliest appearing time of each hashtag. \n",
    "        ## (add an extra column of 1 to mark as topical, will be used in a join later)\n",
    "        ordered_hashtag_set = df_birthday.\\\n",
    "                              groupby(\"each_hashtag\").\\\n",
    "                              agg({\"create_time\": \"min\"}).\\\n",
    "                              orderBy('min(create_time)', ascending=True).\\\n",
    "                              withColumnRenamed(\"min(create_time)\", \"birthday\").\\\n",
    "                              where(df_birthday.each_hashtag.isin(topic_lables)).cache()\n",
    "                            \n",
    "        time_span = ordered_hashtag_set.count()\n",
    "\n",
    "        train_valid_split_time = np.floor(np.multiply(time_span, 0.75)).astype(int)\n",
    "        valid_test_split_time = np.floor(np.multiply(time_span, 0.85)).astype(int)\n",
    "\n",
    "        # Converting to Pandas for random row access.\n",
    "        pd_ordered_hashtag_set = ordered_hashtag_set.toPandas()\n",
    "        \n",
    "        # locate the timestamp of the cutoff point. Will be used later to split Dataframe.\n",
    "        train_valid_time = pd_ordered_hashtag_set.iloc[train_valid_split_time]['birthday']\n",
    "        valid_test_time = pd_ordered_hashtag_set.iloc[valid_test_split_time]['birthday']\n",
    "    \n",
    "        training_set = labled_data.where(col(\"create_time\") <= train_valid_time)\n",
    "        validation_set = labled_data.where((col(\"create_time\") > train_valid_time) \\\n",
    "                                           & (col(\"create_time\") <= valid_test_time))\n",
    "        test_set = labled_data.where(col(\"create_time\") > valid_test_time)\n",
    "        \n",
    "        if remove_topic_hashtags == True:\n",
    "            train_hashtags = pd_ordered_hashtag_set[:train_valid_split_time]['each_hashtag'].tolist()\n",
    "            valid_hashtags = pd_ordered_hashtag_set[train_valid_split_time:valid_test_split_time]['each_hashtag']\\\n",
    "                                .tolist()\n",
    "            test_hashtags = pd_ordered_hashtag_set[valid_test_split_time:]['each_hashtag'].tolist()\n",
    "            \n",
    "            valid_test_hashtags = valid_hashtags + test_hashtags\n",
    "            hashtags_filter = [re.sub(r'(.*)', r'\\\\b\\1\\\\b', hashtag) for hashtag in valid_test_hashtags]\n",
    "            valid_test_hashtags_regex = '|'.join(hashtags_filter)\n",
    "            if ch == True:\n",
    "                invalid_train_ids = (training_set.filter(lower(training_set['hashtag']).rlike(valid_test_hashtags_regex)))\\\n",
    "                                        .select(\"tweet_id\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "            else:\n",
    "                invalid_train_ids = (training_set.filter(training_set['hashtag'].rlike(valid_test_hashtags_regex)))\\\n",
    "                                        .select(\"tweet_id\").distinct().rdd.flatMap(lambda x: x).collect()                \n",
    "            training_set = training_set.where(~col(\"tweet_id\").isin(invalid_train_ids)) \n",
    "\n",
    "            train_test_hashtags = train_hashtags + test_hashtags\n",
    "            hashtags_filter = [re.sub(r'(.*)', r'\\\\b\\1\\\\b', hashtag) for hashtag in train_test_hashtags]\n",
    "            train_test_hashtags_regex = '|'.join(hashtags_filter)\n",
    "            if ch == True:\n",
    "                invalid_validation_ids = (validation_set.filter(lower(validation_set['hashtag']).rlike(train_test_hashtags_regex)))\\\n",
    "                                        .select(\"tweet_id\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "            else:\n",
    "                invalid_validation_ids = (validation_set.filter(validation_set['hashtag'].rlike(train_test_hashtags_regex)))\\\n",
    "                                        .select(\"tweet_id\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "                \n",
    "            print(\"Invalid Validation:\" + str(len(invalid_validation_ids)))\n",
    "            validation_set = validation_set.where(~col(\"tweet_id\").isin(invalid_validation_ids))\n",
    "            \n",
    "            train_valid_hashtags = train_hashtags + valid_hashtags\n",
    "            train_valid_hashtags_filter = [re.sub(r'(.*)', r'\\\\b\\1\\\\b', hashtag) for hashtag in train_valid_hashtags]\n",
    "            train_valid_hashtags_regex = '|'.join(train_valid_hashtags_filter)\n",
    "            if ch == True:\n",
    "                invalid_test_ids = (test_set.filter(lower(test_set['hashtag']).rlike(train_valid_hashtags_regex)))\\\n",
    "                                        .select(\"tweet_id\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "            else:\n",
    "                invalid_test_ids = (test_set.filter(test_set['hashtag'].rlike(train_test_hashtags_regex)))\\\n",
    "                                        .select(\"tweet_id\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "                \n",
    "            print(\"Invalid Test:\" + str(len(invalid_test_ids)))\n",
    "                \n",
    "            test_set = test_set.where(~col(\"tweet_id\").isin(invalid_test_ids))\n",
    "        \n",
    "        if balance_data == True:\n",
    "            train_pos_count = training_set.where(col(\"label\") == 1).count()\n",
    "            train_neg_count = training_set.where(col(\"label\") == 0).count()\n",
    "            train_pos_neg_ratio = float(train_pos_count) / train_neg_count\n",
    "            \n",
    "            training_set_balanced = training_set.sampleBy(\"label\", fractions={0: 2*train_pos_neg_ratio, 1: 1}, seed=seed)\n",
    "            \n",
    "            valid_pos_count = validation_set.where(col(\"label\") == 1).count()\n",
    "            valid_neg_count = validation_set.count() - valid_pos_count\n",
    "            valid_pos_neg_ratio = float(valid_pos_count) / valid_neg_count\n",
    "            \n",
    "            validation_set_balanced = validation_set.sampleBy(\"label\", fractions={0.0: valid_pos_neg_ratio, 1.0: 1}, seed=seed)\n",
    "            \n",
    "            return training_set, training_set_balanced, validation_set, validation_set_balanced, test_set\n",
    "            \n",
    "        return training_set, validation_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_selected_features(topic, tf, test, num_feat=50):\n",
    "\n",
    "    domain_stopwords = ['empty_location', 'empty_mention']#,'rt', 'amp', 'http']\n",
    "    positive_set, negative_set = get_transformed_data(test, tf.get_pipeline(),\\\n",
    "                                                      stopwords=domain_stopwords)\n",
    "\n",
    "    num_positive, feature_positive_coverage, num_negative, feature_negative_coverage = \\\n",
    "        get_coverage(tf.get_pipeline(), positive_set, negative_set)\n",
    "        \n",
    "    positive_set_subset, negative_set_subset = subset_transformed_data(positive_set, 2500, negative_set=negative_set)\n",
    "\n",
    "    num_positive_subset, feature_positive_coverage_subset, num_negative_subset, feature_negative_coverage_subset = \\\n",
    "        get_coverage(tf.get_pipeline(), positive_set_subset, negative_set_subset)\n",
    "    \n",
    "    print(\"Subsetted\")\n",
    "    \n",
    "    baseline = topk_features(tf.get_pipeline(), k=num_feat + 20)\n",
    "    baseline = baseline[20:]\n",
    "\n",
    "    basic = max_cover(feature_positive_coverage_subset, num_positive_subset, k=num_feat)\n",
    "    print(\"basic\")\n",
    "    if len(test) > 3000:\n",
    "        test_subset = test.sample(n=3000, random_state=123)\n",
    "    else:\n",
    "        test_subset = test\n",
    "\n",
    "    mi_scores = get_mi_scores(test_subset, tf.get_pipeline(), feature_positive_coverage)    \n",
    "    num_tweets = len(test_subset)\n",
    "    num_positives = len(test_subset[test_subset.label == 1])\n",
    "    mi = max_cover_with_mutual_information_normalized(feature_positive_coverage_subset, \\\n",
    "                                                                        mi_scores, num_positive_subset, \\\n",
    "                                                                        num_positive_subset, k=num_feat)\n",
    "    print(\"mi\")\n",
    "    greedy = greedy_max_cover(positive_set, negative_set, tf.get_pipeline(), k=num_feat)\n",
    "\n",
    "    solver = max_cover_with_negs_unweighted(feature_positive_coverage_subset,\\\n",
    "                               feature_negative_coverage_subset,\\\n",
    "                               num_positive_subset, num_negative_subset,\\\n",
    "                               k=num_feat, time_limit_secs=600)\n",
    "\n",
    "    return baseline, basic, mi, greedy, solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human_Disaster\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (0,1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1d53a70c0b55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtrain_pd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemporal_split_directory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/training2.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mvalidation_pd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemporal_split_directory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/validation2.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtest_pd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemporal_split_directory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/test2.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_pd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'term'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    643\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m     \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    936\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 938\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'as_recarray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1505\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1508\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.read (pandas/parser.c:10364)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._read_low_memory (pandas/parser.c:10640)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._read_rows (pandas/parser.c:11677)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._convert_column_data (pandas/parser.c:13111)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._convert_tokens (pandas/parser.c:14065)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._convert_with_dtype (pandas/parser.c:14732)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/site-packages/pandas/types/common.pyc\u001b[0m in \u001b[0;36mis_integer_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_integer_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0mtipo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_dtype_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     return (issubclass(tipo, np.integer) and\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "temporal_split_directory = '/mnt/1e69d2b1-91a9-473c-a164-db90daf43a3d/splitted_data/'\n",
    "topics = [\"Human_Disaster\", \"Tennis\", \"LGBT\",\"Health\", \"Social_issue\", \"Space\", \"Soccer\"]\n",
    "#topics = [ \"LGBT\",\"Health\", \"Social_issue\", \"Space\", ]\n",
    "#\"Natr_Disaster\",\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "    train_pd = pd.read_csv(temporal_split_directory + topic + \"/training2.csv\").dropna()\n",
    "    validation_pd = pd.read_csv(temporal_split_directory + topic + \"/validation2.csv\").dropna()\n",
    "    test_pd = pd.read_csv(temporal_split_directory + topic + \"/test2.csv\").dropna().reset_index()\n",
    "\n",
    "    test = test_pd.drop_duplicates(subset='term').reset_index()\n",
    "    valid = validation_pd.drop_duplicates(subset='term').reset_index()\n",
    "    train = train_pd.drop_duplicates(subset='term').reset_index()\n",
    "\n",
    "    tf = TestFramework(train, valid, test)\n",
    "    \n",
    "    baseline, basic, mi, greedy, solver = get_selected_features(topic, tf, test)\n",
    "    \n",
    "    _, _, ideal_avep, _ = tf.get_ideal_performance()\n",
    "    \n",
    "    #filtered_baseline = tf.get_filtered_data_by_index(baseline)\n",
    "    #_, _, baseline_avep, _ = tf.run_with_filtered_test_data(filtered_baseline) \n",
    "    \n",
    "    filtered_basic = tf.get_filtered_data_by_index(basic)\n",
    "    _, _, basic_avep, _ = tf.run_with_filtered_test_data(filtered_basic)\n",
    "    \n",
    "    filtered_mi = tf.get_filtered_data_by_index(mi)\n",
    "    _, _, mi_avep, _ = tf.run_with_filtered_test_data(filtered_mi)\n",
    "    \n",
    "    filtered_greedy = tf.get_filtered_data_by_index(greedy)\n",
    "    _, _, greedy_avep, _ = tf.run_with_filtered_test_data(filtered_greedy)\n",
    "    \n",
    "    filtered_solver = tf.get_filtered_data_by_index(solver)\n",
    "    _, _, solver_avep, _ = tf.run_with_filtered_test_data(filtered_solver)\n",
    "    \n",
    "    \n",
    "    results_pd = pd.DataFrame()\n",
    "\n",
    "    # Ideal\n",
    "    pos = len(test[test.label == 1])\n",
    "    tot = len(test)\n",
    "    print(\"Number of Topical Tweets w/o duplicates= {0}\".format(pos))\n",
    "    print(\"Number of Tweets w/o duplicates = {0}\".format(tot))\n",
    "\n",
    "    results_pd = results_pd.append([[\"Ideal\", pos, tot, 1.0, (float)(pos) / tot, ideal_avep]])\n",
    "\n",
    "    # Baseline\n",
    "    #pos_ret = len(filtered_baseline[filtered_baseline.label == 1])\n",
    "    #tot_ret = len(filtered_baseline)\n",
    "    #recall = (float)(pos_ret) / pos\n",
    "\n",
    "    #results_pd = results_pd.append([[\"Baseline\", pos_ret, tot_ret\\\n",
    "    #                                 , recall, (float)(pos_ret) / tot_ret, baseline_avep]])\n",
    "\n",
    "    # Basic LP\n",
    "    pos_ret = len(filtered_basic[filtered_basic.label == 1])\n",
    "    tot_ret = len(filtered_basic)\n",
    "    recall = (float)(pos_ret) / pos\n",
    "\n",
    "    results_pd = results_pd.append([[\"Basic LP Formulation\", pos_ret, tot_ret\\\n",
    "                                     , recall, (float)(pos_ret) / tot_ret, basic_avep]])\n",
    "\n",
    "    # MI \n",
    "    pos_ret = len(filtered_mi[filtered_mi.label == 1])\n",
    "    tot_ret = len(filtered_mi)\n",
    "    recall = (float)(pos_ret) / pos\n",
    "\n",
    "    results_pd = results_pd.append([[\"LP Formulation With Mutual Information\", pos_ret, tot_ret\\\n",
    "                                     , recall, (float)(pos_ret) / tot_ret, mi_avep]])\n",
    "    \n",
    "    # Greedy \n",
    "    pos_ret = len(filtered_greedy[filtered_greedy.label == 1])\n",
    "    tot_ret = len(filtered_greedy)\n",
    "    recall = (float)(pos_ret) / pos\n",
    "\n",
    "    results_pd = results_pd.append([[\"LP with Negative Coverage (Greedy Solver)\", pos_ret, tot_ret\\\n",
    "                                     , recall, (float)(pos_ret) / tot_ret, greedy_avep]])\n",
    "    \n",
    "    # Solver \n",
    "    pos_ret = len(filtered_solver[filtered_solver.label == 1])\n",
    "    tot_ret = len(filtered_solver)\n",
    "    recall = (float)(pos_ret) / pos\n",
    "\n",
    "    results_pd = results_pd.append([[\"LP with Negative Coverage (Gurobi Solver)\", pos_ret, tot_ret\\\n",
    "                                     , recall, (float)(pos_ret) / tot_ret, solver_avep]])    \n",
    "    \n",
    "    results_pd.columns = [\"Model\", \"Topical Retrieved\", \"Total Retrieved\", \"Recall\", \"Precision\", \"AveP\"]\n",
    "    \n",
    "    print(topic)\n",
    "    print(results_pd.to_latex())\n",
    "    \n",
    "    #0.949866"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soccer\n",
      "Initializing Classifier....\n",
      "Complete!\n",
      "#pos/#neg = 0.00506089887743\n",
      "12254\n",
      "24508\n",
      "transform done!\n",
      "Subsetted\n",
      "basic\n",
      "mi\n",
      "Number of Topical Tweets w/o duplicates= 12254\n",
      "Number of Tweets w/o duplicates = 2433563\n",
      "Soccer\n",
      "\\begin{tabular}{llrrrrr}\n",
      "\\toprule\n",
      "{} &                                      Model &  Topical Retrieved &  Total Retrieved &    Recall &  Precision &      AveP \\\\\n",
      "\\midrule\n",
      "0 &                                      Ideal &              12254 &          2433563 &  1.000000 &   0.005035 &  0.040087 \\\\\n",
      "0 &                                   Baseline &                 43 &              472 &  0.003509 &   0.091102 &  0.261755 \\\\\n",
      "0 &                       Basic LP Formulation &               8203 &           886806 &  0.669414 &   0.009250 &  0.059153 \\\\\n",
      "0 &     LP Formulation With Mutual Information &               7742 &           818448 &  0.631794 &   0.009459 &  0.062986 \\\\\n",
      "0 &  LP with Negative Coverage (Greedy Solver) &               6942 &           220222 &  0.566509 &   0.031523 &  0.087788 \\\\\n",
      "0 &  LP with Negative Coverage (Gurobi Solver) &               6274 &           121297 &  0.511996 &   0.051724 &  0.098457 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temporal_split_directory = '/mnt/1e69d2b1-91a9-473c-a164-db90daf43a3d/splitted_data/'\n",
    "topics = [\"Soccer\"]\n",
    "\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "    train_pd = pd.read_csv(temporal_split_directory + topic + \"/training2.csv\").dropna()\n",
    "    validation_pd = pd.read_csv(temporal_split_directory + topic + \"/validation2.csv\").dropna()\n",
    "    test_pd = pd.read_csv(temporal_split_directory + topic + \"/test2.csv\").dropna().reset_index()\n",
    "\n",
    "    test = test_pd.drop_duplicates(subset='term').reset_index()\n",
    "    valid = validation_pd.drop_duplicates(subset='term').reset_index()\n",
    "    train = train_pd.drop_duplicates(subset='term').reset_index()\n",
    "\n",
    "    tf = TestFramework(train, valid, test)\n",
    "    \n",
    "    baseline, basic, mi, greedy, solver = get_selected_features(topic, tf, test)\n",
    "    \n",
    "    _, _, ideal_avep, _ = tf.get_ideal_performance()\n",
    "    \n",
    "    filtered_baseline = tf.get_filtered_data_by_index(baseline)\n",
    "    _, _, baseline_avep, _ = tf.run_with_filtered_test_data(filtered_baseline) \n",
    "    \n",
    "    filtered_basic = tf.get_filtered_data_by_index(basic)\n",
    "    _, _, basic_avep, _ = tf.run_with_filtered_test_data(filtered_basic)\n",
    "    \n",
    "    filtered_mi = tf.get_filtered_data_by_index(mi)\n",
    "    _, _, mi_avep, _ = tf.run_with_filtered_test_data(filtered_mi)\n",
    "    \n",
    "    filtered_greedy = tf.get_filtered_data_by_index(greedy)\n",
    "    _, _, greedy_avep, _ = tf.run_with_filtered_test_data(filtered_greedy)\n",
    "    \n",
    "    filtered_solver = tf.get_filtered_data_by_index(solver)\n",
    "    _, _, solver_avep, _ = tf.run_with_filtered_test_data(filtered_solver)\n",
    "    \n",
    "    \n",
    "    results_pd = pd.DataFrame()\n",
    "\n",
    "    # Ideal\n",
    "    pos = len(test[test.label == 1])\n",
    "    tot = len(test)\n",
    "    print(\"Number of Topical Tweets w/o duplicates= {0}\".format(pos))\n",
    "    print(\"Number of Tweets w/o duplicates = {0}\".format(tot))\n",
    "\n",
    "    results_pd = results_pd.append([[\"Ideal\", pos, tot, 1.0, (float)(pos) / tot, ideal_avep]])\n",
    "\n",
    "    # Baseline\n",
    "    pos_ret = len(filtered_baseline[filtered_baseline.label == 1])\n",
    "    tot_ret = len(filtered_baseline)\n",
    "    recall = (float)(pos_ret) / pos\n",
    "\n",
    "    results_pd = results_pd.append([[\"Baseline\", pos_ret, tot_ret\\\n",
    "                                     , recall, (float)(pos_ret) / tot_ret, baseline_avep]])\n",
    "\n",
    "    # Basic LP\n",
    "    pos_ret = len(filtered_basic[filtered_basic.label == 1])\n",
    "    tot_ret = len(filtered_basic)\n",
    "    recall = (float)(pos_ret) / pos\n",
    "\n",
    "    results_pd = results_pd.append([[\"Basic LP Formulation\", pos_ret, tot_ret\\\n",
    "                                     , recall, (float)(pos_ret) / tot_ret, basic_avep]])\n",
    "\n",
    "    # MI \n",
    "    pos_ret = len(filtered_mi[filtered_mi.label == 1])\n",
    "    tot_ret = len(filtered_mi)\n",
    "    recall = (float)(pos_ret) / pos\n",
    "\n",
    "    results_pd = results_pd.append([[\"LP Formulation With Mutual Information\", pos_ret, tot_ret\\\n",
    "                                     , recall, (float)(pos_ret) / tot_ret, mi_avep]])\n",
    "    \n",
    "    # Greedy \n",
    "    pos_ret = len(filtered_greedy[filtered_greedy.label == 1])\n",
    "    tot_ret = len(filtered_greedy)\n",
    "    recall = (float)(pos_ret) / pos\n",
    "\n",
    "    results_pd = results_pd.append([[\"LP with Negative Coverage (Greedy Solver)\", pos_ret, tot_ret\\\n",
    "                                     , recall, (float)(pos_ret) / tot_ret, greedy_avep]])\n",
    "    \n",
    "    # Solver \n",
    "    pos_ret = len(filtered_solver[filtered_solver.label == 1])\n",
    "    tot_ret = len(filtered_solver)\n",
    "    recall = (float)(pos_ret) / pos\n",
    "\n",
    "    results_pd = results_pd.append([[\"LP with Negative Coverage (Gurobi Solver)\", pos_ret, tot_ret\\\n",
    "                                     , recall, (float)(pos_ret) / tot_ret, solver_avep]])    \n",
    "    \n",
    "    results_pd.columns = [\"Model\", \"Topical Retrieved\", \"Total Retrieved\", \"Recall\", \"Precision\", \"AveP\"]\n",
    "    \n",
    "    print(topic)\n",
    "    print(results_pd.to_latex())\n",
    "    \n",
    "    #0.949866"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (0,1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/usr/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Classifier....\n",
      "Complete!\n",
      "cov begin\n",
      "#pos/#neg = 0.000238417915043\n",
      "491\n",
      "982\n",
      "transform done!\n",
      "Coverage done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python2.7/site-packages/scipy/sparse/compressed.py:774: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  SparseEfficiencyWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MI done\n",
      "Tennis\n",
      "\\begin{tabular}{llrrrrr}\n",
      "\\toprule\n",
      "{} &  Model &  Topical Retrieved &  Total Retrieved &    Recall &  Precision &      AveP \\\\\n",
      "\\midrule\n",
      "0 &   CILP &                393 &           568264 &  0.800407 &   0.000692 &  0.035997 \\\\\n",
      "0 &   WILP &                393 &           568264 &  0.800407 &   0.000692 &  0.035997 \\\\\n",
      "0 &  CAILP &                358 &           154982 &  0.729124 &   0.002310 &  0.047435 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Classifier....\n",
      "Complete!\n",
      "cov begin\n",
      "#pos/#neg = 0.00113502198081\n",
      "970\n",
      "1940\n",
      "transform done!\n",
      "Coverage done\n",
      "MI done\n",
      "Health\n",
      "\\begin{tabular}{llrrrrr}\n",
      "\\toprule\n",
      "{} &  Model &  Topical Retrieved &  Total Retrieved &    Recall &  Precision &      AveP \\\\\n",
      "\\midrule\n",
      "0 &   CILP &                702 &           305196 &  0.723711 &   0.002300 &  0.010576 \\\\\n",
      "0 &   WILP &                702 &           305196 &  0.723711 &   0.002300 &  0.010576 \\\\\n",
      "0 &  CAILP &                618 &           104669 &  0.637113 &   0.005904 &  0.019160 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "Initializing Classifier....\n",
      "Complete!\n",
      "cov begin\n",
      "#pos/#neg = 0.000473015924282\n",
      "1611\n",
      "3222\n",
      "transform done!\n",
      "Coverage done\n",
      "MI done\n",
      "LGBT\n",
      "\\begin{tabular}{llrrrrr}\n",
      "\\toprule\n",
      "{} &  Model &  Topical Retrieved &  Total Retrieved &    Recall &  Precision &      AveP \\\\\n",
      "\\midrule\n",
      "0 &   CILP &                914 &          1478121 &  0.567349 &   0.000618 &  0.001090 \\\\\n",
      "0 &   WILP &                914 &          1478121 &  0.567349 &   0.000618 &  0.001090 \\\\\n",
      "0 &  CAILP &                666 &           329518 &  0.413408 &   0.002021 &  0.003002 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temporal_split_directory = '/mnt/1e69d2b1-91a9-473c-a164-db90daf43a3d/splitted_data/'\n",
    "topics = [\"Tennis\", \"Health\", \"LGBT\"]\n",
    "#\"Natr_Disaster\", \"Social_issue\", \"Space\", \"Soccer\", \"Human_Disaster\", \n",
    "for topic in topics:\n",
    "    train_pd = pd.read_csv(temporal_split_directory + topic + \"/training2.csv\").dropna()\n",
    "    validation_pd = pd.read_csv(temporal_split_directory + topic + \"/validation2.csv\").dropna()\n",
    "    test_pd = pd.read_csv(temporal_split_directory + topic + \"/test2.csv\").dropna().reset_index()\n",
    "\n",
    "    test = test_pd.drop_duplicates(subset='term').reset_index()\n",
    "    valid = validation_pd.drop_duplicates(subset='term').reset_index()\n",
    "    train = train_pd.drop_duplicates(subset='term').reset_index()\n",
    "\n",
    "    tf = TestFramework(train, valid, test)\n",
    "\n",
    "    domain_stopwords = ['empty_location', 'empty_mention','rt', 'amp', 'http']\n",
    "    print(\"cov begin\")\n",
    "    positive_set, negative_set = get_transformed_data(test, tf.get_pipeline(),\\\n",
    "                                                      stopwords=domain_stopwords)\n",
    "    \n",
    "    num_positive, feature_positive_coverage, num_negative, feature_negative_coverage = \\\n",
    "        get_coverage(tf.get_pipeline(), positive_set, negative_set)\n",
    "\n",
    "    print(\"Coverage done\")   \n",
    "    if len(test) > 6000:\n",
    "        test_subset = test.sample(n=6000, random_state=123)\n",
    "    else:\n",
    "        test_subset = test\n",
    "\n",
    "    mi_scores = get_mi_scores(test_subset, tf.get_pipeline(), feature_positive_coverage)    \n",
    "    \n",
    "    print(\"MI done\")   \n",
    "    num_tweets = len(test_subset)\n",
    "    num_positives = len(test_subset[test_subset.label == 1])\n",
    "    cilp = greedy_cilp(positive_set, tf.get_pipeline(), k=50)\n",
    "    wilp = greedy_wilp(positive_set, mi_scores, tf.get_pipeline(), k=50)\n",
    "    cailp = greedy_max_cover(positive_set, negative_set, tf.get_pipeline(), k=50)\n",
    "    #print(\"methods ran\")\n",
    "    filtered_cilp = tf.get_filtered_data_by_index(cilp)\n",
    "    #print(\"cilp filtered\")\n",
    "    _, _, cilp_avep, _ = tf.run_with_filtered_test_data(filtered_cilp)\n",
    "    #print(\"cilp avep\")\n",
    "    filtered_wilp = tf.get_filtered_data_by_index(wilp)\n",
    "    #print(\"wilp filtered\")\n",
    "    _, _, wilp_avep, _ = tf.run_with_filtered_test_data(filtered_wilp)    \n",
    "    #print(\"wilp avep\")\n",
    "\n",
    "    filtered_cailp = tf.get_filtered_data_by_index(cailp)\n",
    "    #print(\"cailp filtered\")\n",
    "    _, _, cailp_avep, _ = tf.run_with_filtered_test_data(filtered_cailp)\n",
    "    #print(\"cailp avp\")\n",
    "\n",
    "    results_pd = pd.DataFrame()\n",
    "\n",
    "    pos = len(test[test.label == 1])\n",
    "    tot = len(test)\n",
    "\n",
    "    # CILP\n",
    "    pos_ret = len(filtered_cilp[filtered_cilp.label == 1])\n",
    "    tot_ret = len(filtered_cilp)\n",
    "    recall = (float)(pos_ret) / pos\n",
    "\n",
    "    results_pd = results_pd.append([[\"CILP\", pos_ret, tot_ret\\\n",
    "                                     , recall, (float)(pos_ret) / tot_ret, cilp_avep]])\n",
    "\n",
    "    # WILP\n",
    "    pos_ret = len(filtered_wilp[filtered_wilp.label == 1])\n",
    "    tot_ret = len(filtered_wilp)\n",
    "    recall = (float)(pos_ret) / pos\n",
    "\n",
    "    results_pd = results_pd.append([[\"WILP\", pos_ret, tot_ret\\\n",
    "                                     , recall, (float)(pos_ret) / tot_ret, wilp_avep]])\n",
    "\n",
    "    # CAILP\n",
    "    pos_ret = len(filtered_cailp[filtered_cailp.label == 1])\n",
    "    tot_ret = len(filtered_cailp)\n",
    "    recall = (float)(pos_ret) / pos\n",
    "\n",
    "    results_pd = results_pd.append([[\"CAILP\", pos_ret, tot_ret\\\n",
    "                                     , recall, (float)(pos_ret) / tot_ret, cailp_avep]])\n",
    "        \n",
    "    results_pd.columns = [\"Model\", \"Topical Retrieved\", \"Total Retrieved\", \"Recall\", \"Precision\", \"AveP\"]\n",
    "    \n",
    "    print(topic)\n",
    "    print(results_pd.to_latex())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Effect of Varying K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Classifier....\n",
      "Complete!\n",
      "#pos/#neg = 0.0193934962322\n",
      "4774\n",
      "9548\n",
      "transform done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python2.7/site-packages/scipy/sparse/compressed.py:774: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  SparseEfficiencyWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Classifier....\n",
      "Complete!\n",
      "#pos/#neg = 0.00506089887743\n",
      "12254\n",
      "24508\n",
      "transform done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Classifier....\n",
      "Complete!\n",
      "#pos/#neg = 0.000943669106367\n",
      "2623\n",
      "5246\n",
      "transform done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (0,1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Classifier....\n",
      "Complete!\n",
      "#pos/#neg = 0.008543642424\n",
      "12452\n",
      "24904\n",
      "transform done!\n",
      "Initializing Classifier....\n",
      "Complete!\n",
      "#pos/#neg = 0.000238417915043\n",
      "491\n",
      "982\n",
      "transform done!\n",
      "Initializing Classifier....\n",
      "Complete!\n",
      "#pos/#neg = 0.00113502198081\n",
      "970\n",
      "1940\n",
      "transform done!\n",
      "Initializing Classifier....\n",
      "Complete!\n",
      "#pos/#neg = 0.000473015924282\n",
      "1611\n",
      "3222\n",
      "transform done!\n",
      "Initializing Classifier....\n",
      "Complete!\n",
      "#pos/#neg = 0.00115122440869\n",
      "1179\n",
      "2358\n",
      "transform done!\n",
      "\\begin{tabular}{llrrrrr}\n",
      "\\toprule\n",
      "{} &               K &  Topical Retrieved &  Total Retrieved &    Recall &  Precision &      AveP \\\\\n",
      "\\midrule\n",
      "0 &              10 &             1913.0 &           9218.0 &  0.400712 &   0.207529 &  0.377452 \\\\\n",
      "0 &              20 &             2394.0 &          12289.0 &  0.501466 &   0.194808 &  0.352256 \\\\\n",
      "0 &              50 &             3133.0 &          25110.0 &  0.656263 &   0.124771 &  0.230567 \\\\\n",
      "0 &             100 &             3578.0 &          35998.0 &  0.749476 &   0.099394 &  0.192682 \\\\\n",
      "0 &             200 &             3993.0 &          56899.0 &  0.836406 &   0.070177 &  0.145151 \\\\\n",
      "0 &    Social\\_issue &                NaN &              NaN &       NaN &        NaN &       NaN \\\\\n",
      "0 &              10 &             4186.0 &          94268.0 &  0.341603 &   0.044405 &  0.109731 \\\\\n",
      "0 &              20 &             5458.0 &         150471.0 &  0.445406 &   0.036273 &  0.101396 \\\\\n",
      "0 &              50 &             6942.0 &         220222.0 &  0.566509 &   0.031523 &  0.087788 \\\\\n",
      "0 &             100 &             8035.0 &         310391.0 &  0.655704 &   0.025887 &  0.078214 \\\\\n",
      "0 &             200 &             8839.0 &         403642.0 &  0.721315 &   0.021898 &  0.073354 \\\\\n",
      "0 &          Soccer &                NaN &              NaN &       NaN &        NaN &       NaN \\\\\n",
      "0 &              10 &              981.0 &          40704.0 &  0.373999 &   0.024101 &  0.143730 \\\\\n",
      "0 &              20 &             1224.0 &          71961.0 &  0.466641 &   0.017009 &  0.109230 \\\\\n",
      "0 &              50 &             1517.0 &         202331.0 &  0.578345 &   0.007498 &  0.053590 \\\\\n",
      "0 &             100 &             1793.0 &         407594.0 &  0.683568 &   0.004399 &  0.036361 \\\\\n",
      "0 &             200 &             2023.0 &         631986.0 &  0.771254 &   0.003201 &  0.026111 \\\\\n",
      "0 &           Space &                NaN &              NaN &       NaN &        NaN &       NaN \\\\\n",
      "0 &              10 &             3570.0 &          16067.0 &  0.286701 &   0.222195 &  0.622242 \\\\\n",
      "0 &              20 &             5566.0 &          27794.0 &  0.446996 &   0.200259 &  0.472227 \\\\\n",
      "0 &              50 &             7425.0 &          60847.0 &  0.596290 &   0.122027 &  0.402345 \\\\\n",
      "0 &             100 &             8865.0 &         140024.0 &  0.711934 &   0.063311 &  0.313645 \\\\\n",
      "0 &             200 &             9984.0 &         238902.0 &  0.801799 &   0.041791 &  0.293311 \\\\\n",
      "0 &  Human\\_Disaster &                NaN &              NaN &       NaN &        NaN &       NaN \\\\\n",
      "0 &              10 &              234.0 &          62392.0 &  0.476578 &   0.003750 &  0.062959 \\\\\n",
      "0 &              20 &              304.0 &          75262.0 &  0.619145 &   0.004039 &  0.061600 \\\\\n",
      "0 &              50 &              358.0 &         154982.0 &  0.729124 &   0.002310 &  0.047435 \\\\\n",
      "0 &             100 &              403.0 &         271790.0 &  0.820774 &   0.001483 &  0.040363 \\\\\n",
      "0 &             200 &              438.0 &         469103.0 &  0.892057 &   0.000934 &  0.034012 \\\\\n",
      "0 &          Tennis &                NaN &              NaN &       NaN &        NaN &       NaN \\\\\n",
      "0 &              10 &              375.0 &          30669.0 &  0.386598 &   0.012227 &  0.061829 \\\\\n",
      "0 &              20 &              482.0 &          65265.0 &  0.496907 &   0.007385 &  0.024814 \\\\\n",
      "0 &              50 &              618.0 &         104669.0 &  0.637113 &   0.005904 &  0.019160 \\\\\n",
      "0 &             100 &              714.0 &         138825.0 &  0.736082 &   0.005143 &  0.014440 \\\\\n",
      "0 &             200 &              805.0 &         182725.0 &  0.829897 &   0.004406 &  0.011689 \\\\\n",
      "0 &          Health &                NaN &              NaN &       NaN &        NaN &       NaN \\\\\n",
      "0 &              10 &              373.0 &         137256.0 &  0.231533 &   0.002718 &  0.003582 \\\\\n",
      "0 &              20 &              503.0 &         225910.0 &  0.312228 &   0.002227 &  0.003536 \\\\\n",
      "0 &              50 &              706.0 &         375583.0 &  0.438237 &   0.001880 &  0.002793 \\\\\n",
      "0 &             100 &              842.0 &         560819.0 &  0.522657 &   0.001501 &  0.002209 \\\\\n",
      "0 &             200 &              938.0 &         741800.0 &  0.582247 &   0.001264 &  0.001911 \\\\\n",
      "0 &            LGBT &                NaN &              NaN &       NaN &        NaN &       NaN \\\\\n",
      "0 &              10 &              437.0 &          30422.0 &  0.370653 &   0.014365 &  0.107617 \\\\\n",
      "0 &              20 &              605.0 &          61135.0 &  0.513147 &   0.009896 &  0.090832 \\\\\n",
      "0 &              50 &              754.0 &          88801.0 &  0.639525 &   0.008491 &  0.071642 \\\\\n",
      "0 &             100 &              892.0 &         122425.0 &  0.756573 &   0.007286 &  0.055660 \\\\\n",
      "0 &             200 &              985.0 &         173606.0 &  0.835454 &   0.005674 &  0.050464 \\\\\n",
      "0 &   Natr\\_Disaster &                NaN &              NaN &       NaN &        NaN &       NaN \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temporal_split_directory = '/mnt/1e69d2b1-91a9-473c-a164-db90daf43a3d/splitted_data/'\n",
    "topics = [\"Social_issue\", \"Soccer\", \"Space\", \"Human_Disaster\", \"Tennis\", \"Health\", \"LGBT\", \"Natr_Disaster\"]\n",
    "ks = [10, 20, 50, 100, 200]\n",
    "\n",
    "results_pd = pd.DataFrame()\n",
    "\n",
    "for topic in topics:\n",
    "    train_pd = pd.read_csv(temporal_split_directory + topic + \"/training2.csv\").dropna()\n",
    "    validation_pd = pd.read_csv(temporal_split_directory + topic + \"/validation2.csv\").dropna()\n",
    "    test_pd = pd.read_csv(temporal_split_directory + topic + \"/test2.csv\").dropna().reset_index()\n",
    "\n",
    "    test = test_pd.drop_duplicates(subset='term').reset_index()\n",
    "    valid = validation_pd.drop_duplicates(subset='term').reset_index()\n",
    "    train = train_pd.drop_duplicates(subset='term').reset_index()\n",
    "\n",
    "    tf = TestFramework(train, valid, test)\n",
    "    \n",
    "    domain_stopwords = ['rt', 'amp', 'empty_location', 'empty_mention']\n",
    "    positive_set, negative_set = get_transformed_data(test, tf.get_pipeline(),\\\n",
    "                                                      stopwords=domain_stopwords)\n",
    "\n",
    "    num_positive, feature_positive_coverage, num_negative, feature_negative_coverage = \\\n",
    "        get_coverage(tf.get_pipeline(), positive_set, negative_set)\n",
    "\n",
    "    pos = len(test[test.label == 1])\n",
    "    tot = len(test)\n",
    "    \n",
    "    for k in ks:\n",
    "        greedy = greedy_max_cover(positive_set, negative_set, tf.get_pipeline(), k=k)    \n",
    "        filtered_greedy = tf.get_filtered_data_by_index(greedy) \n",
    "        \n",
    "        _, _, avep, _ = tf.run_with_filtered_test_data(filtered_greedy)\n",
    "        \n",
    "        # Greedy \n",
    "        pos_ret = len(filtered_greedy[filtered_greedy.label == 1])\n",
    "        tot_ret = len(filtered_greedy)\n",
    "        recall = (float)(pos_ret) / pos\n",
    "\n",
    "        results_pd = results_pd.append([[k, pos_ret, tot_ret\\\n",
    "                                         , recall, (float)(pos_ret) / tot_ret, avep]])\n",
    "        \n",
    "    \n",
    "    results_pd = results_pd.append([[topic]])\n",
    "    \n",
    "results_pd.columns = [\"K\", \"Topical Retrieved\", \"Total Retrieved\", \"Recall\", \"Precision\", \"AveP\"]    \n",
    "print(results_pd.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results_pd.to_csv(\"vark.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>AveP</th>\n",
       "      <th>K</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Topical Retrieved</th>\n",
       "      <th>Total Retrieved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.377452</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.207529</td>\n",
       "      <td>0.400712</td>\n",
       "      <td>1913.0</td>\n",
       "      <td>9218.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.352256</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.194808</td>\n",
       "      <td>0.501466</td>\n",
       "      <td>2394.0</td>\n",
       "      <td>12289.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.230567</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.124771</td>\n",
       "      <td>0.656263</td>\n",
       "      <td>3133.0</td>\n",
       "      <td>25110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.192682</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.099394</td>\n",
       "      <td>0.749476</td>\n",
       "      <td>3578.0</td>\n",
       "      <td>35998.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.145151</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.070177</td>\n",
       "      <td>0.836406</td>\n",
       "      <td>3993.0</td>\n",
       "      <td>56899.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.109731</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.044405</td>\n",
       "      <td>0.341603</td>\n",
       "      <td>4186.0</td>\n",
       "      <td>94268.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.101396</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.036273</td>\n",
       "      <td>0.445406</td>\n",
       "      <td>5458.0</td>\n",
       "      <td>150471.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.087788</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.031523</td>\n",
       "      <td>0.566509</td>\n",
       "      <td>6942.0</td>\n",
       "      <td>220222.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.078214</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.025887</td>\n",
       "      <td>0.655704</td>\n",
       "      <td>8035.0</td>\n",
       "      <td>310391.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.073354</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.021898</td>\n",
       "      <td>0.721315</td>\n",
       "      <td>8839.0</td>\n",
       "      <td>403642.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200.0</td>\n",
       "      <td>8839.0</td>\n",
       "      <td>403642.0</td>\n",
       "      <td>0.721315</td>\n",
       "      <td>0.021898</td>\n",
       "      <td>0.073354</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1         2         3         4         5      AveP      K  \\\n",
       "0    NaN     NaN       NaN       NaN       NaN       NaN  0.377452   10.0   \n",
       "0    NaN     NaN       NaN       NaN       NaN       NaN  0.352256   20.0   \n",
       "0    NaN     NaN       NaN       NaN       NaN       NaN  0.230567   50.0   \n",
       "0    NaN     NaN       NaN       NaN       NaN       NaN  0.192682  100.0   \n",
       "0    NaN     NaN       NaN       NaN       NaN       NaN  0.145151  200.0   \n",
       "0    NaN     NaN       NaN       NaN       NaN       NaN  0.109731   10.0   \n",
       "0    NaN     NaN       NaN       NaN       NaN       NaN  0.101396   20.0   \n",
       "0    NaN     NaN       NaN       NaN       NaN       NaN  0.087788   50.0   \n",
       "0    NaN     NaN       NaN       NaN       NaN       NaN  0.078214  100.0   \n",
       "0    NaN     NaN       NaN       NaN       NaN       NaN  0.073354  200.0   \n",
       "0  200.0  8839.0  403642.0  0.721315  0.021898  0.073354       NaN    NaN   \n",
       "\n",
       "   Precision    Recall  Topical Retrieved  Total Retrieved  \n",
       "0   0.207529  0.400712             1913.0           9218.0  \n",
       "0   0.194808  0.501466             2394.0          12289.0  \n",
       "0   0.124771  0.656263             3133.0          25110.0  \n",
       "0   0.099394  0.749476             3578.0          35998.0  \n",
       "0   0.070177  0.836406             3993.0          56899.0  \n",
       "0   0.044405  0.341603             4186.0          94268.0  \n",
       "0   0.036273  0.445406             5458.0         150471.0  \n",
       "0   0.031523  0.566509             6942.0         220222.0  \n",
       "0   0.025887  0.655704             8035.0         310391.0  \n",
       "0   0.021898  0.721315             8839.0         403642.0  \n",
       "0        NaN       NaN                NaN              NaN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_pd.append([[k, pos_ret, tot_ret\\\n",
    "                                         , recall, (float)(pos_ret) / tot_ret, avep]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Selected Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natr_Disaster\n",
      "Initializing Classifier....\n",
      "Complete!\n",
      "#pos/#neg = 0.00115122440869\n",
      "1179\n",
      "2358\n",
      "transform done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python2.7/site-packages/scipy/sparse/compressed.py:774: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  SparseEfficiencyWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetted\n",
      "basic\n",
      "mi\n",
      "#pos/#neg = 0.00115122440869\n",
      "1179\n",
      "2358\n",
      "transform done!\n",
      "Subsetted\n",
      "basic\n",
      "mi\n",
      "#pos/#neg = 0.00115122440869\n",
      "1179\n",
      "2358\n",
      "transform done!\n",
      "Subsetted\n",
      "basic\n",
      "mi\n",
      "#pos/#neg = 0.00115122440869\n",
      "1179\n",
      "2358\n",
      "transform done!\n",
      "Subsetted\n",
      "basic\n",
      "mi\n",
      "Soccer\n",
      "Initializing Classifier....\n",
      "Complete!\n",
      "#pos/#neg = 0.00506089887743\n",
      "12254\n",
      "24508\n",
      "transform done!\n",
      "Subsetted\n",
      "basic\n",
      "mi\n",
      "#pos/#neg = 0.00506089887743\n",
      "12254\n",
      "24508\n",
      "transform done!\n",
      "Subsetted\n",
      "basic\n",
      "mi\n",
      "#pos/#neg = 0.00506089887743\n",
      "12254\n",
      "24508\n",
      "transform done!\n",
      "Subsetted\n",
      "basic\n",
      "mi\n",
      "#pos/#neg = 0.00506089887743\n",
      "12254\n",
      "24508\n",
      "transform done!\n",
      "Subsetted\n",
      "basic\n",
      "mi\n",
      "Space\n",
      "Initializing Classifier....\n",
      "Complete!\n",
      "#pos/#neg = 0.000943669106367\n",
      "2623\n",
      "5246\n",
      "transform done!\n",
      "Subsetted\n",
      "basic\n",
      "mi\n",
      "#pos/#neg = 0.000943669106367\n",
      "2623\n",
      "5246\n",
      "transform done!\n",
      "Subsetted\n",
      "basic\n",
      "mi\n",
      "#pos/#neg = 0.000943669106367\n",
      "2623\n",
      "5246\n",
      "transform done!\n",
      "Subsetted\n",
      "basic\n",
      "mi\n",
      "#pos/#neg = 0.000943669106367\n",
      "2623\n",
      "5246\n",
      "transform done!\n",
      "Subsetted\n",
      "basic\n",
      "mi\n",
      "Human_Disaster\n",
      "Initializing Classifier....\n",
      "Complete!\n",
      "#pos/#neg = 0.008543642424\n",
      "12452\n",
      "24904\n",
      "transform done!\n",
      "Subsetted\n",
      "basic\n",
      "mi\n",
      "#pos/#neg = 0.008543642424\n",
      "12452\n",
      "24904\n",
      "transform done!\n",
      "Subsetted\n",
      "basic\n",
      "mi\n",
      "#pos/#neg = 0.008543642424\n",
      "12452\n",
      "24904\n",
      "transform done!\n",
      "Subsetted\n",
      "basic\n",
      "mi\n",
      "#pos/#neg = 0.008543642424\n",
      "12452\n",
      "24904\n",
      "transform done!\n",
      "Subsetted\n",
      "basic\n",
      "mi\n",
      "Tennis\n",
      "Initializing Classifier....\n",
      "Complete!\n",
      "#pos/#neg = 0.000238417915043\n",
      "491\n",
      "982\n",
      "transform done!\n",
      "Subsetted\n",
      "basic\n",
      "mi\n",
      "#pos/#neg = 0.000238417915043\n",
      "491\n",
      "982\n",
      "transform done!\n",
      "Subsetted\n",
      "basic\n",
      "mi\n",
      "#pos/#neg = 0.000238417915043\n",
      "491\n",
      "982\n",
      "transform done!\n",
      "Subsetted\n",
      "basic\n",
      "mi\n",
      "#pos/#neg = 0.000238417915043\n",
      "491\n",
      "982\n",
      "transform done!\n",
      "Subsetted\n",
      "basic\n",
      "mi\n",
      "Health\n",
      "Initializing Classifier....\n",
      "Complete!\n",
      "#pos/#neg = 0.00113502198081\n",
      "970\n",
      "1940\n",
      "transform done!\n",
      "Subsetted\n",
      "basic\n",
      "mi\n",
      "#pos/#neg = 0.00113502198081\n",
      "970\n",
      "1940\n",
      "transform done!\n",
      "Subsetted\n",
      "basic\n",
      "mi\n",
      "#pos/#neg = 0.00113502198081\n",
      "970\n",
      "1940\n",
      "transform done!\n",
      "Subsetted\n",
      "basic\n",
      "mi\n",
      "#pos/#neg = 0.00113502198081\n",
      "970\n",
      "1940\n",
      "transform done!\n",
      "Subsetted\n",
      "basic\n",
      "mi\n",
      "LGBT\n",
      "Initializing Classifier....\n",
      "Complete!\n",
      "#pos/#neg = 0.000473015924282\n",
      "1611\n",
      "3222\n",
      "transform done!\n",
      "Subsetted\n",
      "basic\n",
      "mi\n",
      "#pos/#neg = 0.000473015924282\n",
      "1611\n",
      "3222\n",
      "transform done!\n",
      "Subsetted\n",
      "basic\n",
      "mi\n",
      "#pos/#neg = 0.000473015924282\n",
      "1611\n",
      "3222\n",
      "transform done!\n",
      "Subsetted\n",
      "basic\n",
      "mi\n",
      "#pos/#neg = 0.000473015924282\n",
      "1611\n",
      "3222\n",
      "transform done!\n",
      "Subsetted\n",
      "basic\n",
      "mi\n",
      "Social_issue\n",
      "Initializing Classifier....\n",
      "Complete!\n",
      "#pos/#neg = 0.0193934962322\n",
      "4774\n",
      "9548\n",
      "transform done!\n",
      "Subsetted\n",
      "basic\n",
      "mi\n",
      "#pos/#neg = 0.0193934962322\n",
      "4774\n",
      "9548\n",
      "transform done!\n",
      "Subsetted\n",
      "basic\n",
      "mi\n",
      "#pos/#neg = 0.0193934962322\n",
      "4774\n",
      "9548\n",
      "transform done!\n",
      "Subsetted\n",
      "basic\n",
      "mi\n",
      "#pos/#neg = 0.0193934962322\n",
      "4774\n",
      "9548\n",
      "transform done!\n",
      "Subsetted\n",
      "basic\n",
      "mi\n"
     ]
    }
   ],
   "source": [
    "temporal_split_directory = '/mnt/1e69d2b1-91a9-473c-a164-db90daf43a3d/splitted_data/'\n",
    "topics = [\"Natr_Disaster\", \"Soccer\",\"Space\", \"Human_Disaster\", \"Tennis\", \"Health\", \"LGBT\", \"Social_issue\"]\n",
    "\n",
    "results5_pd = pd.DataFrame()\n",
    "results10_pd = pd.DataFrame()\n",
    "results20_pd = pd.DataFrame()\n",
    "results100_pd = pd.DataFrame()\n",
    "\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "    train_pd = pd.read_csv(temporal_split_directory + topic + \"/training2.csv\").dropna()\n",
    "    validation_pd = pd.read_csv(temporal_split_directory + topic + \"/validation2.csv\").dropna()\n",
    "    test_pd = pd.read_csv(temporal_split_directory + topic + \"/test2.csv\").dropna().reset_index()\n",
    "\n",
    "    test = test_pd.drop_duplicates(subset='term').reset_index()\n",
    "    valid = validation_pd.drop_duplicates(subset='term').reset_index()\n",
    "    train = train_pd.drop_duplicates(subset='term').reset_index()\n",
    "\n",
    "    tf = TestFramework(train, valid, test)\n",
    "    \n",
    "    baseline, basic, mi, _, solver = get_selected_features(topic, tf, test, num_feat=5)\n",
    "    \n",
    "    baseline_features = get_feature_by_index(tf.get_pipeline(), baseline)\n",
    "    basic_features = get_feature_by_index(tf.get_pipeline(), basic)\n",
    "    mi_features = get_feature_by_index(tf.get_pipeline(), mi)\n",
    "#    greedy_features = get_feature_by_index(tf.get_pipeline(), greedy)\n",
    "    solver_features = get_feature_by_index(tf.get_pipeline(), solver)\n",
    "    \n",
    "    results5_pd = results5_pd.append(pd.concat([baseline_features, basic_features, mi_features, solver_features], axis=1))\\\n",
    "    \n",
    "    baseline, basic, mi, _, solver = get_selected_features(topic, tf, test, num_feat=10)\n",
    "    \n",
    "    baseline_features = get_feature_by_index(tf.get_pipeline(), baseline)\n",
    "    basic_features = get_feature_by_index(tf.get_pipeline(), basic)\n",
    "    mi_features = get_feature_by_index(tf.get_pipeline(), mi)\n",
    "#    greedy_features = get_feature_by_index(tf.get_pipeline(), greedy)\n",
    "    solver_features = get_feature_by_index(tf.get_pipeline(), solver)\n",
    "    \n",
    "    results10_pd = results10_pd.append(pd.concat([baseline_features, basic_features, mi_features, solver_features], axis=1))\\\n",
    "    \n",
    "    baseline, basic, mi, _, solver = get_selected_features(topic, tf, test, num_feat=20)\n",
    "    \n",
    "    baseline_features = get_feature_by_index(tf.get_pipeline(), baseline)\n",
    "    basic_features = get_feature_by_index(tf.get_pipeline(), basic)\n",
    "    mi_features = get_feature_by_index(tf.get_pipeline(), mi)\n",
    "#    greedy_features = get_feature_by_index(tf.get_pipeline(), greedy)\n",
    "    solver_features = get_feature_by_index(tf.get_pipeline(), solver)\n",
    "    \n",
    "    results20_pd = results20_pd.append(pd.concat([baseline_features, basic_features, mi_features, solver_features], axis=1))\\\n",
    "    \n",
    "    baseline, basic, mi, _, solver = get_selected_features(topic, tf, test, num_feat=100)\n",
    "    \n",
    "    baseline_features = get_feature_by_index(tf.get_pipeline(), baseline)\n",
    "    basic_features = get_feature_by_index(tf.get_pipeline(), basic)\n",
    "    mi_features = get_feature_by_index(tf.get_pipeline(), mi)\n",
    "#    greedy_features = get_feature_by_index(tf.get_pipeline(), greedy)\n",
    "    solver_features = get_feature_by_index(tf.get_pipeline(), solver)\n",
    "    \n",
    "    results100_pd = results100_pd.append(pd.concat([baseline_features, basic_features, mi_features, solver_features], axis=1))\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results5_pd.to_csv(\"feats5.csv\", encoding='utf-8', index=False)\n",
    "results10_pd.to_csv(\"feats10.csv\", encoding='utf-8', index=False)\n",
    "results20_pd.to_csv(\"feats20.csv\", encoding='utf-8', index=False)\n",
    "results100_pd.to_csv(\"feats100.csv\", encoding='utf-8', index=False)\n",
    "#results_pd.to_csv(\"feats100.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results_pd.to_csv(\"feats100.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results20_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results100_pd.iloc[:800].to_csv(\"feats100.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = results100_pd.iloc[:800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r.to_csv(\"feats100.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Type</th>\n",
       "      <th>Feature.1</th>\n",
       "      <th>Type.1</th>\n",
       "      <th>Feature.2</th>\n",
       "      <th>Type.2</th>\n",
       "      <th>Feature.3</th>\n",
       "      <th>Type.3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hurricanesandy</td>\n",
       "      <td>Hashtag</td>\n",
       "      <td>aerial</td>\n",
       "      <td>Term</td>\n",
       "      <td>aerial</td>\n",
       "      <td>Term</td>\n",
       "      <td>61</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>earthquakeph</td>\n",
       "      <td>Hashtag</td>\n",
       "      <td>aftermath</td>\n",
       "      <td>Term</td>\n",
       "      <td>aftermath</td>\n",
       "      <td>Term</td>\n",
       "      <td>aap</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abfloods</td>\n",
       "      <td>Hashtag</td>\n",
       "      <td>afternoon</td>\n",
       "      <td>Term</td>\n",
       "      <td>alive</td>\n",
       "      <td>Term</td>\n",
       "      <td>affected</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>philippines</td>\n",
       "      <td>Term</td>\n",
       "      <td>alive</td>\n",
       "      <td>Term</td>\n",
       "      <td>allah</td>\n",
       "      <td>Term</td>\n",
       "      <td>aftermath</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>philippines</td>\n",
       "      <td>Hashtag</td>\n",
       "      <td>allah</td>\n",
       "      <td>Term</td>\n",
       "      <td>amp</td>\n",
       "      <td>Term</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>storm</td>\n",
       "      <td>Term</td>\n",
       "      <td>amp</td>\n",
       "      <td>Term</td>\n",
       "      <td>appears</td>\n",
       "      <td>Term</td>\n",
       "      <td>aftershocks</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>typhoon</td>\n",
       "      <td>Term</td>\n",
       "      <td>appears</td>\n",
       "      <td>Term</td>\n",
       "      <td>area</td>\n",
       "      <td>Term</td>\n",
       "      <td>aid</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>victims</td>\n",
       "      <td>Term</td>\n",
       "      <td>area</td>\n",
       "      <td>Term</td>\n",
       "      <td>areas</td>\n",
       "      <td>Term</td>\n",
       "      <td>appears</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>corkfloods</td>\n",
       "      <td>Hashtag</td>\n",
       "      <td>army</td>\n",
       "      <td>Term</td>\n",
       "      <td>army</td>\n",
       "      <td>Term</td>\n",
       "      <td>area</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>earthquake</td>\n",
       "      <td>Term</td>\n",
       "      <td>beautiful</td>\n",
       "      <td>Term</td>\n",
       "      <td>avoid</td>\n",
       "      <td>Term</td>\n",
       "      <td>areas</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>typhoonaid</td>\n",
       "      <td>Hashtag</td>\n",
       "      <td>braces</td>\n",
       "      <td>Term</td>\n",
       "      <td>baby</td>\n",
       "      <td>Term</td>\n",
       "      <td>army</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cholera</td>\n",
       "      <td>Hashtag</td>\n",
       "      <td>bring</td>\n",
       "      <td>Term</td>\n",
       "      <td>beautiful</td>\n",
       "      <td>Term</td>\n",
       "      <td>bridge</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>relief</td>\n",
       "      <td>Term</td>\n",
       "      <td>campaign</td>\n",
       "      <td>Term</td>\n",
       "      <td>books</td>\n",
       "      <td>Term</td>\n",
       "      <td>buildings</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>manuel</td>\n",
       "      <td>Hashtag</td>\n",
       "      <td>cancel</td>\n",
       "      <td>Term</td>\n",
       "      <td>braces</td>\n",
       "      <td>Term</td>\n",
       "      <td>busy</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>affected</td>\n",
       "      <td>Term</td>\n",
       "      <td>cancelled</td>\n",
       "      <td>Term</td>\n",
       "      <td>bring</td>\n",
       "      <td>Term</td>\n",
       "      <td>california</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>typhoonyolanda</td>\n",
       "      <td>Hashtag</td>\n",
       "      <td>children</td>\n",
       "      <td>Term</td>\n",
       "      <td>cancelled</td>\n",
       "      <td>Term</td>\n",
       "      <td>cancel</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>hurricane</td>\n",
       "      <td>Term</td>\n",
       "      <td>college</td>\n",
       "      <td>Term</td>\n",
       "      <td>care</td>\n",
       "      <td>Term</td>\n",
       "      <td>cancelled</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>yolandaph</td>\n",
       "      <td>Hashtag</td>\n",
       "      <td>come</td>\n",
       "      <td>Term</td>\n",
       "      <td>children</td>\n",
       "      <td>Term</td>\n",
       "      <td>collapsed</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>julio</td>\n",
       "      <td>Hashtag</td>\n",
       "      <td>contact</td>\n",
       "      <td>Term</td>\n",
       "      <td>college</td>\n",
       "      <td>Term</td>\n",
       "      <td>contact</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>safe</td>\n",
       "      <td>Term</td>\n",
       "      <td>corner</td>\n",
       "      <td>Term</td>\n",
       "      <td>come</td>\n",
       "      <td>Term</td>\n",
       "      <td>coverage</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>help</td>\n",
       "      <td>Term</td>\n",
       "      <td>country</td>\n",
       "      <td>Term</td>\n",
       "      <td>continues</td>\n",
       "      <td>Term</td>\n",
       "      <td>critical</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>water</td>\n",
       "      <td>Term</td>\n",
       "      <td>damage</td>\n",
       "      <td>Term</td>\n",
       "      <td>corner</td>\n",
       "      <td>Term</td>\n",
       "      <td>damage</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>magnitude</td>\n",
       "      <td>Term</td>\n",
       "      <td>day</td>\n",
       "      <td>Term</td>\n",
       "      <td>country</td>\n",
       "      <td>Term</td>\n",
       "      <td>damaged</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>hurricaneseason</td>\n",
       "      <td>Hashtag</td>\n",
       "      <td>days</td>\n",
       "      <td>Term</td>\n",
       "      <td>damage</td>\n",
       "      <td>Term</td>\n",
       "      <td>delhi</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>cyclone</td>\n",
       "      <td>Term</td>\n",
       "      <td>did</td>\n",
       "      <td>Term</td>\n",
       "      <td>day</td>\n",
       "      <td>Term</td>\n",
       "      <td>delivering</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>flood</td>\n",
       "      <td>Term</td>\n",
       "      <td>donate</td>\n",
       "      <td>Term</td>\n",
       "      <td>days</td>\n",
       "      <td>Term</td>\n",
       "      <td>destroyed</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>quakelive</td>\n",
       "      <td>Hashtag</td>\n",
       "      <td>downtown</td>\n",
       "      <td>Term</td>\n",
       "      <td>did</td>\n",
       "      <td>Term</td>\n",
       "      <td>dirty</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>floodwarning</td>\n",
       "      <td>Hashtag</td>\n",
       "      <td>evacuated</td>\n",
       "      <td>Term</td>\n",
       "      <td>donate</td>\n",
       "      <td>Term</td>\n",
       "      <td>downtown</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>rain</td>\n",
       "      <td>Term</td>\n",
       "      <td>families</td>\n",
       "      <td>Term</td>\n",
       "      <td>downtown</td>\n",
       "      <td>Term</td>\n",
       "      <td>emergency</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>weather</td>\n",
       "      <td>Term</td>\n",
       "      <td>family</td>\n",
       "      <td>Term</td>\n",
       "      <td>evacuated</td>\n",
       "      <td>Term</td>\n",
       "      <td>evacuated</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>family</td>\n",
       "      <td>Term</td>\n",
       "      <td>safe</td>\n",
       "      <td>Term</td>\n",
       "      <td>say</td>\n",
       "      <td>Term</td>\n",
       "      <td>protests</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>day</td>\n",
       "      <td>Term</td>\n",
       "      <td>says</td>\n",
       "      <td>Term</td>\n",
       "      <td>says</td>\n",
       "      <td>Term</td>\n",
       "      <td>race</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>prolife</td>\n",
       "      <td>Hashtag</td>\n",
       "      <td>shit</td>\n",
       "      <td>Term</td>\n",
       "      <td>shit</td>\n",
       "      <td>Term</td>\n",
       "      <td>racism</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>newtown</td>\n",
       "      <td>Term</td>\n",
       "      <td>shut</td>\n",
       "      <td>Term</td>\n",
       "      <td>shoot</td>\n",
       "      <td>Term</td>\n",
       "      <td>racist</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>ceo</td>\n",
       "      <td>Term</td>\n",
       "      <td>sickening</td>\n",
       "      <td>Term</td>\n",
       "      <td>shot</td>\n",
       "      <td>Term</td>\n",
       "      <td>racists</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>prolifeyouth</td>\n",
       "      <td>Mention</td>\n",
       "      <td>silence</td>\n",
       "      <td>Term</td>\n",
       "      <td>sickening</td>\n",
       "      <td>Term</td>\n",
       "      <td>related</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>prevented</td>\n",
       "      <td>Term</td>\n",
       "      <td>son</td>\n",
       "      <td>Term</td>\n",
       "      <td>st</td>\n",
       "      <td>Term</td>\n",
       "      <td>rights</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>debt</td>\n",
       "      <td>Term</td>\n",
       "      <td>state</td>\n",
       "      <td>Term</td>\n",
       "      <td>state</td>\n",
       "      <td>Term</td>\n",
       "      <td>riot</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>dail</td>\n",
       "      <td>Hashtag</td>\n",
       "      <td>stop</td>\n",
       "      <td>Term</td>\n",
       "      <td>statement</td>\n",
       "      <td>Term</td>\n",
       "      <td>rip</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>25</td>\n",
       "      <td>Term</td>\n",
       "      <td>streets</td>\n",
       "      <td>Term</td>\n",
       "      <td>stop</td>\n",
       "      <td>Term</td>\n",
       "      <td>scene</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>sept28</td>\n",
       "      <td>Hashtag</td>\n",
       "      <td>surprised</td>\n",
       "      <td>Term</td>\n",
       "      <td>talking</td>\n",
       "      <td>Term</td>\n",
       "      <td>sharpton</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>laws</td>\n",
       "      <td>Term</td>\n",
       "      <td>talking</td>\n",
       "      <td>Term</td>\n",
       "      <td>tell</td>\n",
       "      <td>Term</td>\n",
       "      <td>shooting</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>assembly</td>\n",
       "      <td>Term</td>\n",
       "      <td>thing</td>\n",
       "      <td>Term</td>\n",
       "      <td>thats</td>\n",
       "      <td>Term</td>\n",
       "      <td>shot</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>wonder</td>\n",
       "      <td>Term</td>\n",
       "      <td>tonight</td>\n",
       "      <td>Term</td>\n",
       "      <td>think</td>\n",
       "      <td>Term</td>\n",
       "      <td>spoken</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>senate</td>\n",
       "      <td>Term</td>\n",
       "      <td>trial</td>\n",
       "      <td>Term</td>\n",
       "      <td>today</td>\n",
       "      <td>Term</td>\n",
       "      <td>statement</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>son</td>\n",
       "      <td>Term</td>\n",
       "      <td>twitter</td>\n",
       "      <td>Term</td>\n",
       "      <td>tonight</td>\n",
       "      <td>Term</td>\n",
       "      <td>streets</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>maryjane</td>\n",
       "      <td>Hashtag</td>\n",
       "      <td>verdict</td>\n",
       "      <td>Term</td>\n",
       "      <td>trial</td>\n",
       "      <td>Term</td>\n",
       "      <td>stupid</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>lougreenwald</td>\n",
       "      <td>Mention</td>\n",
       "      <td>video</td>\n",
       "      <td>Term</td>\n",
       "      <td>video</td>\n",
       "      <td>Term</td>\n",
       "      <td>surprised</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>smoking</td>\n",
       "      <td>Term</td>\n",
       "      <td>world</td>\n",
       "      <td>Term</td>\n",
       "      <td>violence</td>\n",
       "      <td>Term</td>\n",
       "      <td>trial</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>votes</td>\n",
       "      <td>Term</td>\n",
       "      <td>yall</td>\n",
       "      <td>Term</td>\n",
       "      <td>want</td>\n",
       "      <td>Term</td>\n",
       "      <td>tumblr</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>colorado</td>\n",
       "      <td>Term</td>\n",
       "      <td>yrs</td>\n",
       "      <td>Term</td>\n",
       "      <td>way</td>\n",
       "      <td>Term</td>\n",
       "      <td>upset</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>spent</td>\n",
       "      <td>Term</td>\n",
       "      <td>cnn</td>\n",
       "      <td>Hashtag</td>\n",
       "      <td>white</td>\n",
       "      <td>Term</td>\n",
       "      <td>verdict</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>dems</td>\n",
       "      <td>Term</td>\n",
       "      <td>darrenwilson</td>\n",
       "      <td>Hashtag</td>\n",
       "      <td>wilson</td>\n",
       "      <td>Term</td>\n",
       "      <td>violence</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>weed</td>\n",
       "      <td>Hashtag</td>\n",
       "      <td>nypd</td>\n",
       "      <td>Hashtag</td>\n",
       "      <td>wish</td>\n",
       "      <td>Term</td>\n",
       "      <td>weapon</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>marijuana</td>\n",
       "      <td>Term</td>\n",
       "      <td>obama</td>\n",
       "      <td>Hashtag</td>\n",
       "      <td>world</td>\n",
       "      <td>Term</td>\n",
       "      <td>wilson</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>weed</td>\n",
       "      <td>Term</td>\n",
       "      <td>trending</td>\n",
       "      <td>Hashtag</td>\n",
       "      <td>youre</td>\n",
       "      <td>Term</td>\n",
       "      <td>young</td>\n",
       "      <td>Term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>abortions</td>\n",
       "      <td>Term</td>\n",
       "      <td>twitter</td>\n",
       "      <td>Hashtag</td>\n",
       "      <td>nypd</td>\n",
       "      <td>Hashtag</td>\n",
       "      <td>darrenwilson</td>\n",
       "      <td>Hashtag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>marijuana</td>\n",
       "      <td>Hashtag</td>\n",
       "      <td>loc_washington_dc</td>\n",
       "      <td>Location</td>\n",
       "      <td>obama</td>\n",
       "      <td>Hashtag</td>\n",
       "      <td>johncrawford</td>\n",
       "      <td>Hashtag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>cannabis</td>\n",
       "      <td>Term</td>\n",
       "      <td>barackobama</td>\n",
       "      <td>Mention</td>\n",
       "      <td>tcot</td>\n",
       "      <td>Hashtag</td>\n",
       "      <td>nypd</td>\n",
       "      <td>Hashtag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>guns</td>\n",
       "      <td>Term</td>\n",
       "      <td>thereval</td>\n",
       "      <td>Mention</td>\n",
       "      <td>thereval</td>\n",
       "      <td>Mention</td>\n",
       "      <td>oakland</td>\n",
       "      <td>Hashtag</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Feature     Type          Feature.1    Type.1  Feature.2  \\\n",
       "0     hurricanesandy  Hashtag             aerial      Term     aerial   \n",
       "1       earthquakeph  Hashtag          aftermath      Term  aftermath   \n",
       "2           abfloods  Hashtag          afternoon      Term      alive   \n",
       "3        philippines     Term              alive      Term      allah   \n",
       "4        philippines  Hashtag              allah      Term        amp   \n",
       "5              storm     Term                amp      Term    appears   \n",
       "6            typhoon     Term            appears      Term       area   \n",
       "7            victims     Term               area      Term      areas   \n",
       "8         corkfloods  Hashtag               army      Term       army   \n",
       "9         earthquake     Term          beautiful      Term      avoid   \n",
       "10        typhoonaid  Hashtag             braces      Term       baby   \n",
       "11           cholera  Hashtag              bring      Term  beautiful   \n",
       "12            relief     Term           campaign      Term      books   \n",
       "13            manuel  Hashtag             cancel      Term     braces   \n",
       "14          affected     Term          cancelled      Term      bring   \n",
       "15    typhoonyolanda  Hashtag           children      Term  cancelled   \n",
       "16         hurricane     Term            college      Term       care   \n",
       "17         yolandaph  Hashtag               come      Term   children   \n",
       "18             julio  Hashtag            contact      Term    college   \n",
       "19              safe     Term             corner      Term       come   \n",
       "20              help     Term            country      Term  continues   \n",
       "21             water     Term             damage      Term     corner   \n",
       "22         magnitude     Term                day      Term    country   \n",
       "23   hurricaneseason  Hashtag               days      Term     damage   \n",
       "24           cyclone     Term                did      Term        day   \n",
       "25             flood     Term             donate      Term       days   \n",
       "26         quakelive  Hashtag           downtown      Term        did   \n",
       "27      floodwarning  Hashtag          evacuated      Term     donate   \n",
       "28              rain     Term           families      Term   downtown   \n",
       "29           weather     Term             family      Term  evacuated   \n",
       "..               ...      ...                ...       ...        ...   \n",
       "770           family     Term               safe      Term        say   \n",
       "771              day     Term               says      Term       says   \n",
       "772          prolife  Hashtag               shit      Term       shit   \n",
       "773          newtown     Term               shut      Term      shoot   \n",
       "774              ceo     Term          sickening      Term       shot   \n",
       "775     prolifeyouth  Mention            silence      Term  sickening   \n",
       "776        prevented     Term                son      Term         st   \n",
       "777             debt     Term              state      Term      state   \n",
       "778             dail  Hashtag               stop      Term  statement   \n",
       "779               25     Term            streets      Term       stop   \n",
       "780           sept28  Hashtag          surprised      Term    talking   \n",
       "781             laws     Term            talking      Term       tell   \n",
       "782         assembly     Term              thing      Term      thats   \n",
       "783           wonder     Term            tonight      Term      think   \n",
       "784           senate     Term              trial      Term      today   \n",
       "785              son     Term            twitter      Term    tonight   \n",
       "786         maryjane  Hashtag            verdict      Term      trial   \n",
       "787     lougreenwald  Mention              video      Term      video   \n",
       "788          smoking     Term              world      Term   violence   \n",
       "789            votes     Term               yall      Term       want   \n",
       "790         colorado     Term                yrs      Term        way   \n",
       "791            spent     Term                cnn   Hashtag      white   \n",
       "792             dems     Term       darrenwilson   Hashtag     wilson   \n",
       "793             weed  Hashtag               nypd   Hashtag       wish   \n",
       "794        marijuana     Term              obama   Hashtag      world   \n",
       "795             weed     Term           trending   Hashtag      youre   \n",
       "796        abortions     Term            twitter   Hashtag       nypd   \n",
       "797        marijuana  Hashtag  loc_washington_dc  Location      obama   \n",
       "798         cannabis     Term        barackobama   Mention       tcot   \n",
       "799             guns     Term           thereval   Mention   thereval   \n",
       "\n",
       "      Type.2     Feature.3   Type.3  \n",
       "0       Term            61     Term  \n",
       "1       Term           aap     Term  \n",
       "2       Term      affected     Term  \n",
       "3       Term     aftermath     Term  \n",
       "4       Term    aftershock     Term  \n",
       "5       Term   aftershocks     Term  \n",
       "6       Term           aid     Term  \n",
       "7       Term       appears     Term  \n",
       "8       Term          area     Term  \n",
       "9       Term         areas     Term  \n",
       "10      Term          army     Term  \n",
       "11      Term        bridge     Term  \n",
       "12      Term     buildings     Term  \n",
       "13      Term          busy     Term  \n",
       "14      Term    california     Term  \n",
       "15      Term        cancel     Term  \n",
       "16      Term     cancelled     Term  \n",
       "17      Term     collapsed     Term  \n",
       "18      Term       contact     Term  \n",
       "19      Term      coverage     Term  \n",
       "20      Term      critical     Term  \n",
       "21      Term        damage     Term  \n",
       "22      Term       damaged     Term  \n",
       "23      Term         delhi     Term  \n",
       "24      Term    delivering     Term  \n",
       "25      Term     destroyed     Term  \n",
       "26      Term         dirty     Term  \n",
       "27      Term      downtown     Term  \n",
       "28      Term     emergency     Term  \n",
       "29      Term     evacuated     Term  \n",
       "..       ...           ...      ...  \n",
       "770     Term      protests     Term  \n",
       "771     Term          race     Term  \n",
       "772     Term        racism     Term  \n",
       "773     Term        racist     Term  \n",
       "774     Term       racists     Term  \n",
       "775     Term       related     Term  \n",
       "776     Term        rights     Term  \n",
       "777     Term          riot     Term  \n",
       "778     Term           rip     Term  \n",
       "779     Term         scene     Term  \n",
       "780     Term      sharpton     Term  \n",
       "781     Term      shooting     Term  \n",
       "782     Term          shot     Term  \n",
       "783     Term        spoken     Term  \n",
       "784     Term     statement     Term  \n",
       "785     Term       streets     Term  \n",
       "786     Term        stupid     Term  \n",
       "787     Term     surprised     Term  \n",
       "788     Term         trial     Term  \n",
       "789     Term        tumblr     Term  \n",
       "790     Term         upset     Term  \n",
       "791     Term       verdict     Term  \n",
       "792     Term      violence     Term  \n",
       "793     Term        weapon     Term  \n",
       "794     Term        wilson     Term  \n",
       "795     Term         young     Term  \n",
       "796  Hashtag  darrenwilson  Hashtag  \n",
       "797  Hashtag  johncrawford  Hashtag  \n",
       "798  Hashtag          nypd  Hashtag  \n",
       "799  Mention       oakland  Hashtag  \n",
       "\n",
       "[800 rows x 8 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"feats100.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natr_Disaster\n",
      "('Training: ', 49664)\n",
      "('Validation: ', 2830)\n",
      "('Test: ', 1307)\n",
      "Soccer\n",
      "('Training: ', 19140)\n",
      "('Validation: ', 1968)\n",
      "('Test: ', 16570)\n",
      "Space\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training: ', 9963)\n",
      "('Validation: ', 372)\n",
      "('Test: ', 4202)\n",
      "Human_Disaster\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (0,1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training: ', 162206)\n",
      "('Validation: ', 54070)\n",
      "('Test: ', 17513)\n",
      "Tennis\n",
      "('Training: ', 33770)\n",
      "('Validation: ', 442)\n",
      "('Test: ', 508)\n",
      "Health\n",
      "('Training: ', 146110)\n",
      "('Validation: ', 856)\n",
      "('Test: ', 1510)\n",
      "LGBT\n",
      "('Training: ', 2625)\n",
      "('Validation: ', 6)\n",
      "('Test: ', 2004)\n",
      "Social_issue\n",
      "('Training: ', 28102)\n",
      "('Validation: ', 3086)\n",
      "('Test: ', 7529)\n"
     ]
    }
   ],
   "source": [
    "temporal_split_directory = '/mnt/1e69d2b1-91a9-473c-a164-db90daf43a3d/splitted_data/'\n",
    "topics = [\"Natr_Disaster\", \"Soccer\",\"Space\", \"Human_Disaster\", \"Tennis\", \"Health\", \"LGBT\", \"Social_issue\"]\n",
    "\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "    train_pd = pd.read_csv(temporal_split_directory + topic + \"/training2.csv\").dropna()\n",
    "    validation_pd = pd.read_csv(temporal_split_directory + topic + \"/validation2.csv\").dropna()\n",
    "    test_pd = pd.read_csv(temporal_split_directory + topic + \"/test2.csv\").dropna().reset_index()\n",
    "\n",
    "    print(\"Training: \", len(train_pd[train_pd.label == 1]))\n",
    "    print(\"Validation: \", len(validation_pd[validation_pd.label == 1]))\n",
    "    print(\"Test: \", len(test_pd[test_pd.label == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
